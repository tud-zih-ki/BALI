{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Longbench Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metrics for: zero_shot - mistral7b_answer - metrics_longbench_test_formatted.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singledoc_qa</th>\n",
       "      <th>multidoc_qa</th>\n",
       "      <th>summarization</th>\n",
       "      <th>fewshot</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>code</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>8.896667</td>\n",
       "      <td>21.913333</td>\n",
       "      <td>6.243333</td>\n",
       "      <td>39.283333</td>\n",
       "      <td>1.895</td>\n",
       "      <td>44.05</td>\n",
       "      <td>20.380278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>18.38</td>\n",
       "      <td>9.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>4.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   singledoc_qa multidoc_qa summarization    fewshot synthetic   code  \\\n",
       "en     8.896667   21.913333      6.243333  39.283333     1.895  44.05   \n",
       "zh        18.38        9.96           0.0       6.75      4.67    NaN   \n",
       "\n",
       "          avg  \n",
       "en  20.380278  \n",
       "zh      7.952  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metrics for: llmlingua2 - mistral7b_answer - metrics_target3000_longbench_test_formatted.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singledoc_qa</th>\n",
       "      <th>multidoc_qa</th>\n",
       "      <th>summarization</th>\n",
       "      <th>fewshot</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>code</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>27.07</td>\n",
       "      <td>28.08</td>\n",
       "      <td>21.38</td>\n",
       "      <td>48.71</td>\n",
       "      <td>5.335</td>\n",
       "      <td>50.855</td>\n",
       "      <td>30.238333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>31.66</td>\n",
       "      <td>9.68</td>\n",
       "      <td>2.6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   singledoc_qa multidoc_qa summarization fewshot synthetic    code        avg\n",
       "en        27.07       28.08         21.38   48.71     5.335  50.855  30.238333\n",
       "zh        31.66        9.68           2.6    13.0      12.0     NaN     13.788"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metrics for: llmlingua2 - mistral7b_answer - metrics_target2000_longbench_test_formatted.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singledoc_qa</th>\n",
       "      <th>multidoc_qa</th>\n",
       "      <th>summarization</th>\n",
       "      <th>fewshot</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>code</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>25.316667</td>\n",
       "      <td>29.28</td>\n",
       "      <td>21.206667</td>\n",
       "      <td>44.286667</td>\n",
       "      <td>4.335</td>\n",
       "      <td>45.63</td>\n",
       "      <td>28.3425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>26.01</td>\n",
       "      <td>10.32</td>\n",
       "      <td>2.31</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   singledoc_qa multidoc_qa summarization    fewshot synthetic   code      avg\n",
       "en    25.316667       29.28     21.206667  44.286667     4.335  45.63  28.3425\n",
       "zh        26.01       10.32          2.31        9.0       9.5    NaN   11.428"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metrics for: origin - mistral7b_answer - metrics_longbench_test_formatted.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singledoc_qa</th>\n",
       "      <th>multidoc_qa</th>\n",
       "      <th>summarization</th>\n",
       "      <th>fewshot</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>code</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>24.13</td>\n",
       "      <td>27.943333</td>\n",
       "      <td>19.843333</td>\n",
       "      <td>62.836667</td>\n",
       "      <td>8.605</td>\n",
       "      <td>67.1</td>\n",
       "      <td>35.076389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>31.72</td>\n",
       "      <td>10.45</td>\n",
       "      <td>7.92</td>\n",
       "      <td>23.42</td>\n",
       "      <td>10.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   singledoc_qa multidoc_qa summarization    fewshot synthetic  code  \\\n",
       "en        24.13   27.943333     19.843333  62.836667     8.605  67.1   \n",
       "zh        31.72       10.45          7.92      23.42     10.22   NaN   \n",
       "\n",
       "          avg  \n",
       "en  35.076389  \n",
       "zh     16.746  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metrics for: llmlingua2_small - mistral7b_answer - metrics_target3000_longbench_test_formatted.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singledoc_qa</th>\n",
       "      <th>multidoc_qa</th>\n",
       "      <th>summarization</th>\n",
       "      <th>fewshot</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>code</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>25.006667</td>\n",
       "      <td>30.07</td>\n",
       "      <td>20.846667</td>\n",
       "      <td>46.056667</td>\n",
       "      <td>10.91</td>\n",
       "      <td>51.99</td>\n",
       "      <td>30.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>25.67</td>\n",
       "      <td>9.83</td>\n",
       "      <td>7.11</td>\n",
       "      <td>6.25</td>\n",
       "      <td>12.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   singledoc_qa multidoc_qa summarization    fewshot synthetic   code  \\\n",
       "en    25.006667       30.07     20.846667  46.056667     10.91  51.99   \n",
       "zh        25.67        9.83          7.11       6.25     12.75    NaN   \n",
       "\n",
       "          avg  \n",
       "en  30.813333  \n",
       "zh     12.322  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metrics for: llmlingua2_small - mistral7b_answer - metrics_target2000_longbench_test_formatted.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singledoc_qa</th>\n",
       "      <th>multidoc_qa</th>\n",
       "      <th>summarization</th>\n",
       "      <th>fewshot</th>\n",
       "      <th>synthetic</th>\n",
       "      <th>code</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>22.823333</td>\n",
       "      <td>29.583333</td>\n",
       "      <td>20.69</td>\n",
       "      <td>41.743333</td>\n",
       "      <td>8.585</td>\n",
       "      <td>47.335</td>\n",
       "      <td>28.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>28.28</td>\n",
       "      <td>10.27</td>\n",
       "      <td>1.81</td>\n",
       "      <td>6.25</td>\n",
       "      <td>12.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   singledoc_qa multidoc_qa summarization    fewshot synthetic    code     avg\n",
       "en    22.823333   29.583333         20.69  41.743333     8.585  47.335   28.46\n",
       "zh        28.28       10.27          1.81       6.25     12.75     NaN  11.872"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "model = [\"llama_3.1_70b_answer\", \"mistral7b_answer\", \"command_r_plus_answer\", \"gpt_4o_answer\", \"gpt_4o_mini_answer\"][1]\n",
    "task_types = {\n",
    "    \"singledoc_qa\": [\"multifieldqa_en\", \"multifieldqa_zh\", \"narrativeqa\", \"qasper\"],\n",
    "    \"multidoc_qa\": [\"hotpotqa\", \"2wikimqa\", \"musique\", \"dureader\"],\n",
    "    \"summarization\": [\"gov_report\", \"qmsum\", \"multi_news\", \"vcsum\"],\n",
    "    \"fewshot\": [\"triviaqa\", \"samsum\", \"trec\", \"lsht\"],\n",
    "    \"synthetic\": [\"passage_count\", \"passage_retrieval_en\", \"passage_retrieval_zh\"],\n",
    "    \"code\": [\"lcc\", \"repobench-p\"],\n",
    "}\n",
    "zh_tasks = [\"dureader\", \"multifieldqa_zh\", \"vcsum\", \"lsht\", \"passage_retrieval_zh\"]\n",
    "for metrics_path in glob.glob(\"../results/longbench/**/metrics_*.json\", recursive=True):\n",
    "    if model not in metrics_path or \"old\" in metrics_path:\n",
    "        continue\n",
    "    print(f'-- Metrics for: {\" - \".join(metrics_path.split(\"/\")[3:])}')\n",
    "    res: dict = json.load(open(metrics_path, \"r\"))\n",
    "    df = pd.DataFrame(columns=task_types.keys(), index=[\"en\", \"zh\"])\n",
    "    for task_type, tasks in task_types.items():\n",
    "        if not any(task in res for task in tasks):\n",
    "            continue\n",
    "        t = [task for task in tasks if task not in zh_tasks]\n",
    "        task_avg = sum(res[task][\"score\"] for task in t) / len(t)\n",
    "        df.loc[\"en\", task_type] = task_avg\n",
    "        t_zh = [task for task in tasks if task in zh_tasks]\n",
    "        if t_zh:\n",
    "            task_avg_zh = sum(res[task][\"score\"] for task in t_zh) / len(t_zh)\n",
    "            df.loc[\"zh\", task_type] = task_avg_zh\n",
    "    df[\"avg\"] = df.mean(axis=1)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix longbench code category scores (after installation of python-Levenshtein)\n",
    "# Calculate per-programming-language scores\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from metrics import code_sim_score\n",
    "\n",
    "\n",
    "def scorer(dataset, predictions, answers, langs, all_classes):\n",
    "    total_score, langs_scores = 0.0, defaultdict(list)\n",
    "    for prediction, ground_truths, lang in zip(predictions, answers, langs):\n",
    "        score = 0.0\n",
    "        for ground_truth in ground_truths:\n",
    "            score = max(score, code_sim_score(prediction, ground_truth, all_classes=all_classes))\n",
    "        total_score += score\n",
    "        langs_scores[lang].append(score)\n",
    "    return {\n",
    "        \"total\": round(100 * total_score / len(predictions), 2),\n",
    "        \"langs\": {lang: round(100 * sum(scores) / len(scores), 2) for lang, scores in langs_scores.items()},\n",
    "    }\n",
    "\n",
    "\n",
    "def eval(load_path):\n",
    "    results: dict[str, dict] = json.load(open(load_path))\n",
    "    predictions, answers, langs = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    all_classes = {}\n",
    "    for data in results.values():\n",
    "        if data[\"task\"] not in [\"repobench-p\", \"lcc\"]:\n",
    "            continue\n",
    "        predictions[data[\"task\"]].append(data[\"pred\"])\n",
    "        answers[data[\"task\"]].append(data[\"answers\"])\n",
    "        langs[data[\"task\"]].append(lb_data[data[\"idx\"]][\"language\"])\n",
    "        all_classes[data[\"task\"]] = data[\"all_classes\"]\n",
    "    scores = {}\n",
    "    for task in predictions.keys():\n",
    "        pred_list, ans_list, lang_list = predictions[task], answers[task], langs[task]\n",
    "        score_dict = scorer(task, pred_list, ans_list, lang_list, all_classes[task])\n",
    "        print(f'Task: {task} - {score_dict[\"total\"]}')\n",
    "        for lang, lang_score in score_dict[\"langs\"].items():\n",
    "            print(f\"  {lang}: {lang_score}\")\n",
    "        scores[task] = {\"score\": score_dict[\"total\"], \"num\": len(pred_list), \"per_lang\": score_dict[\"langs\"]}\n",
    "    score_list = [s[\"score\"] for s in scores.values()]\n",
    "    scores[\"avg\"] = sum(score_list) / len(score_list)\n",
    "    return scores\n",
    "\n",
    "\n",
    "lb_data = json.load(open(\"../results/longbench/origin/longbench_test_formatted.json\"))\n",
    "for answer_file in glob.glob(\"../results/longbench/**/answer_*.json\", recursive=True):\n",
    "    metrics_file = answer_file.replace(\"answer_\", \"metrics_\")\n",
    "    if \"old\" in answer_file or not os.path.exists(metrics_file):\n",
    "        continue\n",
    "    print(f'-- Metrics for: {\" - \".join(answer_file.split(\"/\")[3:])}')\n",
    "    res = eval(answer_file)\n",
    "    metrics = json.load(open(metrics_file))\n",
    "    metrics[\"lcc\"] = res[\"lcc\"]\n",
    "    metrics[\"repobench-p\"] = res[\"repobench-p\"]\n",
    "    json.dump(metrics, open(metrics_file, \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate average prompt lengths (MeetingBank, LongBench, ZeroScrolls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2prompt = {\n",
    "    \"narrativeqa\": \"You are given a story, which can be either a novel or a movie script, and a question. Answer the question asconcisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nStory: {context}\\n\\nNow, answer the question based on the story asconcisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nQuestion: {input}\\n\\nAnswer:\",\n",
    "    \"qasper\": 'You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\\n\\nArticle: {context}\\n\\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\\n\\nQuestion: {input}\\n\\nAnswer:',\n",
    "    \"multifieldqa_en\": \"Read the following text and answer briefly.\\n\\n{context}\\n\\nNow, answer the following question based on the above text, only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n",
    "    \"multifieldqa_zh\": \"阅读以下文字并用中文简短回答：\\n\\n{context}\\n\\n现在请基于上面的文章回答下面的问题，只告诉我答案，不要输出任何其他字词。\\n\\n问题：{input}\\n回答：\",\n",
    "    \"hotpotqa\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n",
    "    \"2wikimqa\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n",
    "    \"musique\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n",
    "    \"dureader\": \"请基于给定的文章回答下述问题。\\n\\n文章：{context}\\n\\n请基于上述文章回答下面的问题。\\n\\n问题：{input}\\n回答：\",\n",
    "    \"gov_report\": \"You are given a report by a government agency. Write a one-page summary of the report.\\n\\nReport:\\n{context}\\n\\nNow, write a one-page summary of the report.\\n\\nSummary:\",\n",
    "    \"qmsum\": \"You are given a meeting transcript and a query containing a question or instruction. Answer the query in one or more sentences.\\n\\nTranscript:\\n{context}\\n\\nNow, answer the query based on the above meeting transcript in one or more sentences.\\n\\nQuery: {input}\\nAnswer:\",\n",
    "    \"multi_news\": \"You are given several news passages. Write a one-page summary of all news. \\n\\nNews:\\n{context}\\n\\nNow, write a one-page summary of all the news.\\n\\nSummary:\",\n",
    "    \"vcsum\": \"下面有一段会议记录，请你阅读后，写一段总结，总结会议的内容。\\n会议记录：\\n{context}\\n\\n会议总结：\",\n",
    "    \"trec\": \"Please determine the type of the question below. Here are some examples of questions.\\n\\n{context}\\n{input}\",\n",
    "    \"triviaqa\": \"Answer the question based on the given passage. Only give me the answer and do not output any other words. The following are some examples.\\n\\n{context}\\n\\n{input}\",\n",
    "    \"samsum\": \"Summarize the dialogue into a few short sentences. The following are some examples.\\n\\n{context}\\n\\n{input}\",\n",
    "    \"lsht\": \"请判断给定新闻的类别，下面是一些例子。\\n\\n{context}\\n{input}\",\n",
    "    \"passage_count\": \"There are some paragraphs below sourced from Wikipedia. Some of them may be duplicates. Please carefully read these paragraphs and determine how many unique paragraphs there are after removing duplicates. In other words, how many non-repeating paragraphs are there in total?\\n\\n{context}\\n\\nPlease enter the final count of unique paragraphs after removing duplicates. The output format should only contain the number, such as 1, 2, 3, and so on.\\n\\nThe final answer is: \",\n",
    "    \"passage_retrieval_en\": 'Here are 30 paragraphs from Wikipedia, along with an abstract. Please determine which paragraph the abstract is from.\\n\\n{context}\\n\\nThe following is an abstract.\\n\\n{input}\\n\\nPlease enter the number of the paragraph that the abstract is from. The answer format must be like \"Paragraph 1\", \"Paragraph 2\", etc.\\n\\nThe answer is: ',\n",
    "    \"passage_retrieval_zh\": '以下是若干段落文字，以及其中一个段落的摘要。请确定给定的摘要出自哪一段。\\n\\n{context}\\n\\n下面是一个摘要\\n\\n{input}\\n\\n请输入摘要所属段落的编号。答案格式必须是\"段落1\"，\"段落2\"等格式\\n\\n答案是：',\n",
    "    \"lcc\": \"Please complete the code given below. \\n{context}Next line of code:\\n\",\n",
    "    \"repobench-p\": \"Please complete the code given below. \\n{context}{input}Next line of code:\\n\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt_longbench(sample: object, load_key: str):\n",
    "    new_sample = {\"input\": sample[\"question\"]}\n",
    "    if load_key is not None:\n",
    "        new_sample[\"context\"] = sample[load_key]\n",
    "    prompt_format = dataset2prompt[sample[\"task\"]]\n",
    "    return prompt_format.format(\n",
    "        context=new_sample[\"context\"] if \"context\" in new_sample else \"\", input=new_sample[\"input\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Longbench ======\n",
      "\n",
      "# zero_shot - 238.6088 - ZH: 285.263\n",
      "EN tasks:\n",
      "narrativeqa: 93.01\n",
      "qasper: 167.45\n",
      "multifieldqa_en: 50.32666666666667\n",
      "hotpotqa: 73.53\n",
      "2wikimqa: 70.85\n",
      "musique: 72.585\n",
      "gov_report: 34.0\n",
      "qmsum: 61.665\n",
      "multi_news: 33.0\n",
      "trec: 28.0\n",
      "triviaqa: 743.6\n",
      "samsum: 167.32\n",
      "passage_count: 90.0\n",
      "passage_retrieval_en: 238.885\n",
      "lcc: 13.0\n",
      "repobench-p: 1011.91\n",
      "\n",
      "ZH tasks:\n",
      "multifieldqa_zh: 80.49\n",
      "dureader: 48.735\n",
      "vcsum: 38.0\n",
      "lsht: 995.145\n",
      "passage_retrieval_zh: 263.945\n",
      "\n",
      "# llmlingua2 - target3000 - 3138.2674666666667 - ZH: 2881.051\n",
      "EN tasks:\n",
      "narrativeqa: 3194.36\n",
      "qasper: 3148.355\n",
      "multifieldqa_en: 2966.6466666666665\n",
      "hotpotqa: 3320.05\n",
      "2wikimqa: 3115.59\n",
      "musique: 3392.595\n",
      "gov_report: 3127.035\n",
      "qmsum: 2887.355\n",
      "multi_news: 2051.835\n",
      "trec: 2710.135\n",
      "triviaqa: 3807.375\n",
      "samsum: 2966.815\n",
      "passage_count: 3399.055\n",
      "passage_retrieval_en: 3514.295\n",
      "lcc: 2278.276\n",
      "repobench-p: 4114.796\n",
      "\n",
      "ZH tasks:\n",
      "multifieldqa_zh: 2801.23\n",
      "dureader: 2639.245\n",
      "vcsum: 2522.745\n",
      "lsht: 3411.495\n",
      "passage_retrieval_zh: 3030.54\n",
      "\n",
      "# llmlingua2 - target2000 - 2277.806666666667 - ZH: 1914.81\n",
      "EN tasks:\n",
      "narrativeqa: 2320.7\n",
      "qasper: 2241.9\n",
      "multifieldqa_en: 2070.08\n",
      "hotpotqa: 2292.0\n",
      "2wikimqa: 2165.04\n",
      "musique: 2339.09\n",
      "gov_report: 2155.625\n",
      "qmsum: 1937.545\n",
      "multi_news: 1662.575\n",
      "trec: 1780.295\n",
      "triviaqa: 2861.04\n",
      "samsum: 2095.9\n",
      "passage_count: 2339.515\n",
      "passage_retrieval_en: 2491.11\n",
      "lcc: 1891.994\n",
      "repobench-p: 3097.598\n",
      "\n",
      "ZH tasks:\n",
      "multifieldqa_zh: 1853.15\n",
      "dureader: 1676.79\n",
      "vcsum: 1569.245\n",
      "lsht: 2455.685\n",
      "passage_retrieval_zh: 2019.18\n",
      "\n",
      "# origin - 10295.369866666666 - ZH: 15225.405\n",
      "EN tasks:\n",
      "narrativeqa: 29872.14\n",
      "qasper: 5089.545\n",
      "multifieldqa_en: 6988.366666666667\n",
      "hotpotqa: 12867.455\n",
      "2wikimqa: 7187.735\n",
      "musique: 15650.14\n",
      "gov_report: 10276.295\n",
      "qmsum: 13916.7\n",
      "multi_news: 2642.12\n",
      "trec: 6785.215\n",
      "triviaqa: 11799.005\n",
      "samsum: 9172.56\n",
      "passage_count: 14988.68\n",
      "passage_retrieval_en: 12535.96\n",
      "lcc: 3178.966\n",
      "repobench-p: 10826.378\n",
      "\n",
      "ZH tasks:\n",
      "multifieldqa_zh: 7355.45\n",
      "dureader: 17641.545\n",
      "vcsum: 16933.715\n",
      "lsht: 26343.065\n",
      "passage_retrieval_zh: 7853.25\n",
      "\n",
      "# llmlingua2_small - target3000 - 3231.1757333333335 - ZH: 4934.433\n",
      "EN tasks:\n",
      "narrativeqa: 3531.22\n",
      "qasper: 3132.05\n",
      "multifieldqa_en: 3041.98\n",
      "hotpotqa: 3077.55\n",
      "2wikimqa: 2972.635\n",
      "musique: 3111.615\n",
      "gov_report: 2946.575\n",
      "qmsum: 2922.855\n",
      "multi_news: 2181.595\n",
      "trec: 2917.95\n",
      "triviaqa: 3658.27\n",
      "samsum: 3068.965\n",
      "passage_count: 3013.86\n",
      "passage_retrieval_en: 3166.72\n",
      "lcc: 2819.548\n",
      "repobench-p: 4620.932\n",
      "\n",
      "ZH tasks:\n",
      "multifieldqa_zh: 4845.72\n",
      "dureader: 4532.645\n",
      "vcsum: 4916.525\n",
      "lsht: 5459.455\n",
      "passage_retrieval_zh: 4917.82\n",
      "\n",
      "# llmlingua2_small - target2000 - 2355.216 - ZH: 3321.398\n",
      "EN tasks:\n",
      "narrativeqa: 2984.1\n",
      "qasper: 2191.965\n",
      "multifieldqa_en: 2103.42\n",
      "hotpotqa: 2101.42\n",
      "2wikimqa: 2027.04\n",
      "musique: 2128.465\n",
      "gov_report: 1993.335\n",
      "qmsum: 1989.115\n",
      "multi_news: 1737.185\n",
      "trec: 1974.67\n",
      "triviaqa: 2735.56\n",
      "samsum: 2141.435\n",
      "passage_count: 2036.46\n",
      "passage_retrieval_en: 2200.55\n",
      "lcc: 2286.114\n",
      "repobench-p: 3450.46\n",
      "\n",
      "ZH tasks:\n",
      "multifieldqa_zh: 3270.4\n",
      "dureader: 2942.73\n",
      "vcsum: 3228.405\n",
      "lsht: 3851.365\n",
      "passage_retrieval_zh: 3314.09\n",
      "\n",
      "====== Zero Scrolls ======\n",
      "\n",
      "# llmlingua2 - target2000 - 2112.5018587360596\n",
      "gov_report: 2073.65\n",
      "summ_screen_fd: 2023.45\n",
      "qmsum: 1919.1\n",
      "squality: 2079.4\n",
      "quality: 2176.6190476190477\n",
      "narrative_qa: 2359.8\n",
      "qasper: 2172.3928571428573\n",
      "musique: 2035.0\n",
      "space_digest: 2186.6\n",
      "book_sum_sort: 2171.15\n",
      "\n",
      "# llmlingua2 - target3000 - 3004.7137546468402\n",
      "gov_report: 3002.55\n",
      "summ_screen_fd: 2986.05\n",
      "qmsum: 2850.0\n",
      "squality: 3016.6\n",
      "quality: 3189.2380952380954\n",
      "narrative_qa: 3164.8\n",
      "qasper: 3062.035714285714\n",
      "musique: 2387.85\n",
      "space_digest: 3152.25\n",
      "book_sum_sort: 3167.95\n",
      "\n",
      "# origin - 9787.765799256505\n",
      "gov_report: 11636.0\n",
      "summ_screen_fd: 9163.4\n",
      "qmsum: 12407.0\n",
      "squality: 6821.5\n",
      "quality: 6518.523809523809\n",
      "narrative_qa: 41165.1\n",
      "qasper: 4935.071428571428\n",
      "musique: 2399.6\n",
      "space_digest: 6514.0\n",
      "book_sum_sort: 7320.8\n",
      "\n",
      "# llmlingua2_small - target2000 - 2106.8401486988846\n",
      "gov_report: 1949.85\n",
      "summ_screen_fd: 2022.55\n",
      "qmsum: 1932.05\n",
      "squality: 1982.75\n",
      "quality: 2167.9523809523807\n",
      "narrative_qa: 3380.1\n",
      "qasper: 2102.035714285714\n",
      "musique: 2034.5\n",
      "space_digest: 1975.8\n",
      "book_sum_sort: 1891.95\n",
      "\n",
      "# llmlingua2_small - target3000 - 3006.6765799256505\n",
      "gov_report: 2870.6\n",
      "summ_screen_fd: 3028.45\n",
      "qmsum: 2850.2\n",
      "squality: 2966.2\n",
      "quality: 3222.6666666666665\n",
      "narrative_qa: 3888.1\n",
      "qasper: 3029.5714285714284\n",
      "musique: 2455.95\n",
      "space_digest: 2985.65\n",
      "book_sum_sort: 2870.85\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# from reproduction.eval_longbench import build_prompt\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "benchmarks = [\n",
    "    \"longbench\",\n",
    "    \"meetingbank_short\",\n",
    "    \"zero_scrolls\",\n",
    "]\n",
    "all_dict = defaultdict(dict)\n",
    "all_num_samples_dict = defaultdict(dict)\n",
    "for benchmark in benchmarks:\n",
    "    print(f'\\n====== {benchmark.replace(\"_\", \" \").title()} ======')\n",
    "    for scenario in glob.glob(f\"../results/{benchmark}/*/*.json\"):\n",
    "        model, target = os.path.dirname(scenario).split(\"/\")[-1], os.path.basename(scenario).split(\"_\")[1]\n",
    "        compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "        data = json.load(open(scenario, \"r\"))\n",
    "        if isinstance(data, dict):\n",
    "            data = list(data.values())\n",
    "        load_key = (\n",
    "            None\n",
    "            if model == \"zero_shot\"\n",
    "            else \"compressed_prompt\" if compression else \"context\" if benchmark == \"longbench\" else \"prompt\"\n",
    "        )\n",
    "        if benchmark == \"longbench\":\n",
    "            lens, lens_zh = defaultdict(int), defaultdict(int)\n",
    "            samples_per_task, samples_per_task_zh = defaultdict(int), defaultdict(int)\n",
    "            en_tasks = [\"multifieldqa_en\", \"narrativeqa\", \"qasper\", \"hotpotqa\", \"2wikimqa\", \"musique\", \"gov_report\", \"qmsum\", \"multi_news\", \"triviaqa\", \"samsum\", \"trec\", \"passage_count\", \"passage_retrieval_en\", \"lcc\", \"repobench-p\"]\n",
    "            zh_tasks = [\"dureader\", \"multifieldqa_zh\", \"vcsum\", \"lsht\", \"passage_retrieval_zh\"]\n",
    "            data_zh = [d for d in data if d[\"task\"] in zh_tasks]\n",
    "            data = [d for d in data if d[\"task\"] in en_tasks]\n",
    "            for d in data:\n",
    "                lens[d[\"task\"]] += len(tokenizer.encode(build_prompt_longbench(d, load_key)))\n",
    "                samples_per_task[d[\"task\"]] += 1\n",
    "            for d in data_zh:\n",
    "                lens_zh[d[\"task\"]] += len(tokenizer.encode(build_prompt_longbench(d, load_key)))\n",
    "                samples_per_task_zh[d[\"task\"]] += 1\n",
    "            avg_len = sum(lens.values()) / len(data)\n",
    "            avg_len_zh = sum(lens_zh.values()) / len(data_zh)\n",
    "            print(f'\\n# {model} - {target + \" - \" if compression else \"\"}{avg_len} - ZH: {avg_len_zh}')\n",
    "            print(\"EN tasks:\")\n",
    "            for task, length in lens.items():\n",
    "                print(f'{task}: {length / samples_per_task[task]}')\n",
    "            print(\"\\nZH tasks:\")\n",
    "            for task, length in lens_zh.items():\n",
    "                print(f'{task}: {length / samples_per_task_zh[task]}')\n",
    "            all_dict[benchmark][scenario] = lens | lens_zh\n",
    "            all_num_samples_dict[benchmark][scenario] = samples_per_task | samples_per_task_zh\n",
    "        elif benchmark == \"zero_scrolls\":\n",
    "            lens, samples_per_task = defaultdict(int), defaultdict(int)\n",
    "            for d in data:\n",
    "                lens[d[\"task\"]] += len(tokenizer.encode(d[load_key]))\n",
    "                samples_per_task[d[\"task\"]] += 1\n",
    "            avg_len = sum(lens.values()) / len(data)\n",
    "            print(f'\\n# {model} - {target + \" - \" if compression else \"\"}{avg_len}')\n",
    "            for task, length in lens.items():\n",
    "                print(f'{task}: {length / samples_per_task[task]}')\n",
    "            all_dict[benchmark][scenario] = lens\n",
    "            all_num_samples_dict[benchmark][scenario] = samples_per_task\n",
    "        else:\n",
    "            avg_len = sum(len(tokenizer.encode(d[load_key])) for d in data) / len(data)\n",
    "            print(f'# {model} - {target + \" - \" if compression else \"\"}{avg_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "benchmarks = [\n",
    "    \"longbench\",\n",
    "    \"zero_scrolls\",\n",
    "]\n",
    "lb_categories = {\n",
    "    \"Single-doc QA\": [\"multifieldqa_en\", \"multifieldqa_zh\", \"narrativeqa\", \"qasper\"],\n",
    "    \"Multi-doc QA\": [\"hotpotqa\", \"2wikimqa\", \"musique\", \"dureader\"],\n",
    "    \"Summarization\": [\"gov_report\", \"qmsum\", \"multi_news\", \"vcsum\"],\n",
    "    \"Few-Shot\": [\"triviaqa\", \"samsum\", \"trec\", \"lsht\"],\n",
    "    \"Synthetic\": [\"passage_count\", \"passage_retrieval_en\", \"passage_retrieval_zh\"],\n",
    "    \"Code\": [\"lcc\", \"repobench-p\"],\n",
    "}\n",
    "lb_tasknames = {\n",
    "    \"narrativeqa\": \"NarrativeQA\",\n",
    "    \"qasper\": \"Qasper\",\n",
    "    \"multifieldqa_en\": \"MultiFieldQA (EN)\",\n",
    "    \"multifieldqa_zh\": \"MultiFieldQA (ZH)\",\n",
    "    \"hotpotqa\": \"HotpotQA\",\n",
    "    \"2wikimqa\": \"2WikiMultihopQA\",\n",
    "    \"musique\": \"MuSiQue\",\n",
    "    \"dureader\": \"DuReader (ZH)\",\n",
    "    \"gov_report\": \"GovReport\",\n",
    "    \"qmsum\": \"QMSum\",\n",
    "    \"multi_news\": \"MultiNews\",\n",
    "    \"vcsum\": \"VCSUM (ZH)\",\n",
    "    \"trec\": \"TREC\",\n",
    "    \"triviaqa\": \"TriviaQA\",\n",
    "    \"samsum\": \"SAMSum\",\n",
    "    \"lsht\": \"LSHT (ZH)\",\n",
    "    \"passage_retrieval_en\": \"PassageRetrieval (EN)\",\n",
    "    \"passage_count\": \"PassageCount\",\n",
    "    \"passage_retrieval_zh\": \"PassageRetrieval (ZH)\",\n",
    "    \"lcc\": \"LCC\",\n",
    "    \"repobench-p\": \"RepoBench-P\",\n",
    "    \"avg\": \"Average\",\n",
    "}\n",
    "zero_scrolls_tasknames = {\n",
    "    \"gov_report\": \"GovReport\",\n",
    "    \"summ_screen_fd\": \"SummScreenFD\",\n",
    "    \"qmsum\": \"QMSum\",\n",
    "    \"qasper\": \"Qasper\",\n",
    "    \"narrative_qa\": \"NarrativeQA\",\n",
    "    \"quality\": \"QuALITY\",\n",
    "    \"musique\": \"MuSiQue\",\n",
    "    \"squality\": \"SQuALITY\",\n",
    "    \"space_digest\": \"SpaceDigest\",\n",
    "    \"book_sum_sort\": \"BookSumSort\",\n",
    "    \"avg\": \"Average\",\n",
    "}\n",
    "zero_scrolls_categories = {\n",
    "    \"Summarization\": [\"gov_report\", \"qmsum\", \"summ_screen_fd\", \"squality\"],\n",
    "    \"QA\": [\"qasper\", \"narrative_qa\", \"musique\", \"quality\"],\n",
    "    \"Data Processing\": [\"space_digest\", \"book_sum_sort\"],\n",
    "}\n",
    "scen = [\n",
    "    \"origin/\",\n",
    "    \"llmlingua2/compression_target3000\",\n",
    "    \"llmlingua2_small/compression_target3000\",\n",
    "    \"llmlingua2/compression_target2000\",\n",
    "    \"llmlingua2_small/compression_target2000\",\n",
    "]\n",
    "benchmarks = {\n",
    "    \"longbench\": {\n",
    "        \"tasks\": [t for ts in lb_categories.values() for t in ts],\n",
    "        \"tasknames\": lb_tasknames,\n",
    "        \"key\": \"score\",\n",
    "    },\n",
    "    \"zero_scrolls\": {\n",
    "        \"tasks\": [t for ts in zero_scrolls_categories.values() for t in ts],\n",
    "        \"tasknames\": zero_scrolls_tasknames,\n",
    "        \"key\": \"zero_scrolls_score\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    outdir = f\"../../ma-thesis/tables/{benchmark}\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    tasks = benchmarks[benchmark][\"tasks\"]\n",
    "    tasknames = benchmarks[benchmark][\"tasknames\"]\n",
    "    num_samples = list(all_num_samples_dict[benchmark].values())[0]\n",
    "\n",
    "    columns = [(\"Original\", \"\")]\n",
    "    for target in [\"Target 3000\", \"Target 2000\"]:\n",
    "        columns.append((target, \"LLMLingua-2\"))\n",
    "        columns.append((target, \"Ratio\"))\n",
    "        columns.append((target, \"LLMLingua-2-small\"))\n",
    "        columns.append((target, \"Ratio-small\"))\n",
    "\n",
    "    df = pd.DataFrame(index=tasks, columns=pd.MultiIndex.from_tuples(columns))\n",
    "\n",
    "    for s, col in zip(\n",
    "        scen,\n",
    "        [\n",
    "            (\"Original\", \"\"),\n",
    "            (\"Target 3000\", \"LLMLingua-2\"),\n",
    "            (\"Target 3000\", \"LLMLingua-2-small\"),\n",
    "            (\"Target 2000\", \"LLMLingua-2\"),\n",
    "            (\"Target 2000\", \"LLMLingua-2-small\"),\n",
    "        ],\n",
    "    ):\n",
    "        lens = [sc for k, sc in all_dict[benchmark].items() if s in k][0]\n",
    "        for task in tasks:\n",
    "            df.loc[task, col] = lens[task] / num_samples[task]\n",
    "        df.loc[\"avg\", col] = sum(lens.values()) / sum(num_samples.values())\n",
    "        if col[0] != \"Original\":\n",
    "            df.loc[:, (col[0], \"Ratio\" if \"-small\" not in col[1] else \"Ratio-small\")] = (\n",
    "                df.loc[:, (\"Original\", \"\")] / df.loc[:, col]\n",
    "            )\n",
    "\n",
    "    df.index = df.index.map(lambda x: tasknames[x])\n",
    "    for col in df.columns:\n",
    "        if col in [\n",
    "            (\"Target 3000\", \"Ratio\"),\n",
    "            (\"Target 3000\", \"Ratio-small\"),\n",
    "            (\"Target 2000\", \"Ratio\"),\n",
    "            (\"Target 2000\", \"Ratio-small\"),\n",
    "        ]:\n",
    "            df[col] = df[col].map(lambda x: f\"{x:.1f}x\" if isinstance(x, float) else x)\n",
    "        else:\n",
    "            df[col] = df[col].map(lambda x: f\"{x:,.0f}\" if isinstance(x, float) else x)\n",
    "    df.columns = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\n",
    "                (\"Target 3000\", \"Ratio\")\n",
    "                if col == (\"Target 3000\", \"Ratio-small\")\n",
    "                else (\"Target 2000\", \"Ratio\") if col == (\"Target 2000\", \"Ratio-small\") else col\n",
    "            )\n",
    "            for col in df.columns\n",
    "        ]\n",
    "    )\n",
    "    tex_table = df.to_latex(float_format=\"%.0f\", multicolumn_format=\"c|\", column_format=\"@{}l|c|cccc|cccc@{}\")\n",
    "\n",
    "    tex_table = tex_table.replace(\"|}{Target 2\", \"}{Target 2\")\n",
    "    tex_table = tex_table.replace(\"Average\", \"\\\\midrule Average\")\n",
    "    for match in [\"Original\", \"Target 3000\", \"Target 2000\"]:\n",
    "        tex_table = tex_table.replace(match, f\"\\\\textbf{{{match}}}\")\n",
    "    pattern = r\"\\\\midrule Average & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) \\\\\\\\\"\n",
    "    repl = r\"\\\\midrule \\\\textbf{Average} & \\\\textbf{\\1} & \\\\textbf{\\2} & \\\\textbf{\\3} & \\\\textbf{\\4} & \\\\textbf{\\5} & \\\\textbf{\\6} & \\\\textbf{\\7} & \\\\textbf{\\8} & \\\\textbf{\\9} \\\\\\\\\"\n",
    "    tex_table = re.sub(pattern, repl, tex_table)\n",
    "\n",
    "    with open(f\"{outdir}/lengths.tex\", \"w\") as f:\n",
    "        f.write(tex_table)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== LongBench SingleDoc Only (Mistral 7B) ======\n",
      "origin - 14619.258181818182 - ZH: 7355.45\n",
      "zero_shot - 108.43818181818182 - ZH: 80.49\n",
      "llmlingua2 - target2000 - 2223.6945454545453 - ZH: 1853.15\n",
      "llmlingua2 - target3000 - 3115.5272727272727 - ZH: 2801.23\n",
      "llmlingua2_small - target2000 - 2455.8654545454547 - ZH: 3270.4\n",
      "llmlingua2_small - target3000 - 3252.6381818181817 - ZH: 4845.72\n"
     ]
    }
   ],
   "source": [
    "# LongBench SingleDoc only (mistral 7b)\n",
    "import json\n",
    "import tiktoken\n",
    "import glob\n",
    "import os\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "print(\"\\n====== LongBench SingleDoc Only (Mistral 7B) ======\")\n",
    "for scenario in glob.glob(f\"../results/longbench/*/*.json\"):\n",
    "    model, target = os.path.dirname(scenario).split(\"/\")[-1], os.path.basename(scenario).split(\"_\")[1]\n",
    "    compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "    data = json.load(open(scenario, \"r\"))\n",
    "    if isinstance(data, dict):\n",
    "        data = list(data.values())\n",
    "    load_key = None if model == \"zero_shot\" else \"compressed_prompt\" if compression else \"context\"\n",
    "    single_doc_tasks = [\"multifieldqa_en\", \"narrativeqa\", \"qasper\"]\n",
    "    single_doc_tasks_zh = [\"multifieldqa_zh\"]\n",
    "    data_en = [d for d in data if d[\"task\"] in single_doc_tasks]\n",
    "    data_zh = [d for d in data if d[\"task\"] in single_doc_tasks_zh]\n",
    "    avg_len = sum(len(tokenizer.encode(build_prompt_longbench(d, load_key))) for d in data_en) / len(data_en)\n",
    "    avg_len_zh = sum(len(tokenizer.encode(build_prompt_longbench(d, load_key))) for d in data_zh) / len(data_zh)\n",
    "    print(f'{model} - {target + \" - \" if compression else \"\"}{avg_len} - ZH: {avg_len_zh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate average prompt lengths (GSM8K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== GSM8K ======\n",
      "origin - 2366\n",
      "llmlingua2 - target160_gsm8k_cot - 157\n",
      "llmlingua2 - target400_gsm8k_cot - 440\n",
      "llmlingua2_small - target160_gsm8k_cot - 159\n",
      "llmlingua2_small - target400_gsm8k_cot - 455\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "get_prompt_len = lambda dem_list: len(tokenizer.encode(\"\\n\\n\".join([\"Question: \" + dem for dem in dem_list])))\n",
    "print(f\"\\n====== GSM8K ======\")\n",
    "for scenario in glob.glob(f\"../results/gsm8k/*/*gsm8k_cot_example*.json\"):\n",
    "    model, target = os.path.dirname(scenario).split(\"/\")[-1], \"_\".join(os.path.basename(scenario).split(\"_\")[1:-1])\n",
    "    compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "    data = json.load(open(scenario, \"r\"))\n",
    "    if isinstance(data, dict):\n",
    "        data = list(data.values())\n",
    "    data = data[0][\"compressed_prompt_list\" if compression else \"prompt_list\"]\n",
    "    print(f'{model} - {target + \" - \" if compression else \"\"}{get_prompt_len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate average prompt lengths (BBH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== BBH ======\n",
      "\n",
      "## llmlingua2 - target200_bbh_cot_examples.json - 175.62962962962962\n",
      "temporal_sequences: 183\n",
      "disambiguation_qa: 179\n",
      "date_understanding: 192\n",
      "tracking_shuffled_objects_three_objects: 163\n",
      "penguins_in_a_table: 172\n",
      "geometric_shapes: 129\n",
      "snarks: 251\n",
      "ruin_names: 183\n",
      "tracking_shuffled_objects_seven_objects: 163\n",
      "tracking_shuffled_objects_five_objects: 163\n",
      "logical_deduction_three_objects: 167\n",
      "hyperbaton: 156\n",
      "logical_deduction_five_objects: 167\n",
      "logical_deduction_seven_objects: 167\n",
      "movie_recommendation: 180\n",
      "salient_translation_error_detection: 186\n",
      "reasoning_about_colored_objects: 168\n",
      "multistep_arithmetic_two: 180\n",
      "navigate: 171\n",
      "dyck_languages: 187\n",
      "word_sorting: 173\n",
      "sports_understanding: 165\n",
      "boolean_expressions: 170\n",
      "object_counting: 167\n",
      "formal_fallacies: 205\n",
      "causal_judgement: 176\n",
      "web_of_lies: 179\n",
      "\n",
      "## llmlingua2 - target300_bbh_cot_examples.json - 253.07407407407408\n",
      "temporal_sequences: 277\n",
      "disambiguation_qa: 264\n",
      "date_understanding: 289\n",
      "tracking_shuffled_objects_three_objects: 221\n",
      "penguins_in_a_table: 253\n",
      "geometric_shapes: 198\n",
      "snarks: 331\n",
      "ruin_names: 267\n",
      "tracking_shuffled_objects_seven_objects: 221\n",
      "tracking_shuffled_objects_five_objects: 221\n",
      "logical_deduction_three_objects: 243\n",
      "hyperbaton: 242\n",
      "logical_deduction_five_objects: 243\n",
      "logical_deduction_seven_objects: 243\n",
      "movie_recommendation: 260\n",
      "salient_translation_error_detection: 266\n",
      "reasoning_about_colored_objects: 244\n",
      "multistep_arithmetic_two: 271\n",
      "navigate: 250\n",
      "dyck_languages: 282\n",
      "word_sorting: 264\n",
      "sports_understanding: 193\n",
      "boolean_expressions: 246\n",
      "object_counting: 247\n",
      "formal_fallacies: 297\n",
      "causal_judgement: 245\n",
      "web_of_lies: 255\n",
      "\n",
      "## origin - 773.9629629629629\n",
      "temporal_sequences: 901\n",
      "disambiguation_qa: 876\n",
      "date_understanding: 461\n",
      "tracking_shuffled_objects_three_objects: 684\n",
      "penguins_in_a_table: 729\n",
      "geometric_shapes: 1816\n",
      "snarks: 685\n",
      "ruin_names: 941\n",
      "tracking_shuffled_objects_seven_objects: 684\n",
      "tracking_shuffled_objects_five_objects: 684\n",
      "logical_deduction_three_objects: 641\n",
      "hyperbaton: 912\n",
      "logical_deduction_five_objects: 641\n",
      "logical_deduction_seven_objects: 641\n",
      "movie_recommendation: 572\n",
      "salient_translation_error_detection: 1330\n",
      "reasoning_about_colored_objects: 671\n",
      "multistep_arithmetic_two: 899\n",
      "navigate: 686\n",
      "dyck_languages: 841\n",
      "word_sorting: 743\n",
      "sports_understanding: 193\n",
      "boolean_expressions: 482\n",
      "object_counting: 446\n",
      "formal_fallacies: 1143\n",
      "causal_judgement: 793\n",
      "web_of_lies: 802\n",
      "\n",
      "## llmlingua2_small - target200_bbh_cot_examples.json - 188.33333333333334\n",
      "temporal_sequences: 181\n",
      "disambiguation_qa: 188\n",
      "date_understanding: 183\n",
      "tracking_shuffled_objects_three_objects: 182\n",
      "penguins_in_a_table: 201\n",
      "geometric_shapes: 209\n",
      "snarks: 243\n",
      "ruin_names: 186\n",
      "tracking_shuffled_objects_seven_objects: 182\n",
      "tracking_shuffled_objects_five_objects: 182\n",
      "logical_deduction_three_objects: 183\n",
      "hyperbaton: 204\n",
      "logical_deduction_five_objects: 183\n",
      "logical_deduction_seven_objects: 183\n",
      "movie_recommendation: 179\n",
      "salient_translation_error_detection: 189\n",
      "reasoning_about_colored_objects: 153\n",
      "multistep_arithmetic_two: 204\n",
      "navigate: 214\n",
      "dyck_languages: 159\n",
      "word_sorting: 194\n",
      "sports_understanding: 191\n",
      "boolean_expressions: 184\n",
      "object_counting: 165\n",
      "formal_fallacies: 194\n",
      "causal_judgement: 183\n",
      "web_of_lies: 186\n",
      "\n",
      "## llmlingua2_small - target300_bbh_cot_examples.json - 275.0740740740741\n",
      "temporal_sequences: 265\n",
      "disambiguation_qa: 271\n",
      "date_understanding: 282\n",
      "tracking_shuffled_objects_three_objects: 272\n",
      "penguins_in_a_table: 278\n",
      "geometric_shapes: 305\n",
      "snarks: 333\n",
      "ruin_names: 264\n",
      "tracking_shuffled_objects_seven_objects: 272\n",
      "tracking_shuffled_objects_five_objects: 272\n",
      "logical_deduction_three_objects: 270\n",
      "hyperbaton: 287\n",
      "logical_deduction_five_objects: 270\n",
      "logical_deduction_seven_objects: 270\n",
      "movie_recommendation: 252\n",
      "salient_translation_error_detection: 283\n",
      "reasoning_about_colored_objects: 248\n",
      "multistep_arithmetic_two: 305\n",
      "navigate: 316\n",
      "dyck_languages: 266\n",
      "word_sorting: 307\n",
      "sports_understanding: 201\n",
      "boolean_expressions: 273\n",
      "object_counting: 241\n",
      "formal_fallacies: 278\n",
      "causal_judgement: 264\n",
      "web_of_lies: 282\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "\n",
    "\n",
    "def build_prompt_bbh(task, key):\n",
    "    prompt = f\"\\n\\n{task[key]}\" if key is not None else \"\"\n",
    "    return f\"{task['instruction']}{prompt}\\n\\n\"\n",
    "\n",
    "\n",
    "print(f\"\\n====== BBH ======\")\n",
    "bbh_dict = {}\n",
    "for scenario in glob.glob(f\"../results/bbh/*/*bbh_cot_examples*.json\"):\n",
    "    if not \"200\" in scenario and not \"300\" in scenario and not \"origin\" in scenario:\n",
    "        continue\n",
    "    model, target = os.path.dirname(scenario).split(\"/\")[-1], \"_\".join(os.path.basename(scenario).split(\"_\")[1:])\n",
    "    compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "    load_key = None if model == \"zero_shot\" else \"compressed_prompt\" if compression else \"prompt\"\n",
    "    data = json.load(open(scenario, \"r\"))\n",
    "    if isinstance(data, dict):\n",
    "        data = list(data.values())\n",
    "    prompt_lens = {}\n",
    "    for task in data:\n",
    "        prompt_lens[task[\"task\"]] = len(tokenizer.encode(build_prompt_bbh(task, load_key)))\n",
    "    print(f'\\n## {model} - {target + \" - \" if compression else \"\"}{sum(prompt_lens.values()) / len(prompt_lens)}')\n",
    "    for task, p_len in prompt_lens.items():\n",
    "        print(f\"{task}: {p_len}\")\n",
    "    bbh_dict[scenario] = prompt_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "outdir = f\"../../ma-thesis/tables/bbh\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "scen = [\n",
    "    \"origin/\",\n",
    "    \"llmlingua2/compression_target300\",\n",
    "    \"llmlingua2_small/compression_target300\",\n",
    "    \"llmlingua2/compression_target200\",\n",
    "    \"llmlingua2_small/compression_target200\",\n",
    "]\n",
    "bbh_tasknames = {\n",
    "    \"multistep_arithmetic_two\": \"Multi-Step Arithmetic\",\n",
    "    \"boolean_expressions\": \"Boolean Expressions\",\n",
    "    \"logical_deduction_three_objects\": \"Logical Deduction (3 Obj.)\",\n",
    "    \"logical_deduction_five_objects\": \"Logical Deduction (5 Obj.)\",\n",
    "    \"logical_deduction_seven_objects\": \"Logical Deduction (7 Obj.)\",\n",
    "    \"geometric_shapes\": \"Geometric Shapes\",\n",
    "    \"dyck_languages\": \"Dyck Languages\",\n",
    "    \"navigate\": \"Navigate\",\n",
    "    \"temporal_sequences\": \"Temporal Sequences\",\n",
    "    \"formal_fallacies\": \"Formal Fallacies\",\n",
    "    \"object_counting\": \"Object Counting\",\n",
    "    \"penguins_in_a_table\": \"Penguins in a Table\",\n",
    "    \"tracking_shuffled_objects_three_objects\": \"Track. 3 Shuffled Obj.\",\n",
    "    \"tracking_shuffled_objects_five_objects\": \"Track. 5 Shuffled Obj.\",\n",
    "    \"tracking_shuffled_objects_seven_objects\": \"Track. 7 Shuffled Obj.\",\n",
    "    \"reasoning_about_colored_objects\": \"Reasoning about Col. Obj.\",\n",
    "    \"web_of_lies\": \"Web of Lies\",\n",
    "    \"word_sorting\": \"Word Sorting\",\n",
    "    \"disambiguation_qa\": \"Disambiguation QA\",\n",
    "    \"hyperbaton\": \"Hyperbaton\",\n",
    "    \"salient_translation_error_detection\": \"Salient Transl. Err. Detection\",\n",
    "    \"snarks\": \"Snarks\",\n",
    "    \"sports_understanding\": \"Sports Understanding\",\n",
    "    \"movie_recommendation\": \"Movie Recommendation\",\n",
    "    \"date_understanding\": \"Date Understanding\",\n",
    "    \"causal_judgement\": \"Causal Judgement\",\n",
    "    \"ruin_names\": \"Ruin Names\",\n",
    "}\n",
    "\n",
    "columns = [(\"Original\", \"\")]\n",
    "for target in [\"Target 300\", \"Target 200\"]:\n",
    "    columns.append((target, \"LLMLingua-2\"))\n",
    "    columns.append((target, \"Ratio\"))\n",
    "    columns.append((target, \"LLMLingua-2-small\"))\n",
    "    columns.append((target, \"Ratio-small\"))\n",
    "\n",
    "df = pd.DataFrame(index=bbh_tasknames.keys(), columns=pd.MultiIndex.from_tuples(columns))\n",
    "\n",
    "for s, col in zip(\n",
    "    scen,\n",
    "    [\n",
    "        (\"Original\", \"\"),\n",
    "        (\"Target 300\", \"LLMLingua-2\"),\n",
    "        (\"Target 300\", \"LLMLingua-2-small\"),\n",
    "        (\"Target 200\", \"LLMLingua-2\"),\n",
    "        (\"Target 200\", \"LLMLingua-2-small\"),\n",
    "    ],\n",
    "):\n",
    "    lens = [sc for k, sc in bbh_dict.items() if s in k][0]\n",
    "    for task in bbh_tasknames:\n",
    "        df.loc[task, col] = lens[task]\n",
    "    df.loc[\"avg\", col] = sum(lens.values()) / len(lens)\n",
    "    if col[0] != \"Original\":\n",
    "        df.loc[:, (col[0], \"Ratio\" if \"-small\" not in col[1] else \"Ratio-small\")] = (\n",
    "            df.loc[:, (\"Original\", \"\")] / df.loc[:, col]\n",
    "        )\n",
    "\n",
    "df.index = df.index.map(lambda x: bbh_tasknames[x] if x != \"avg\" else \"Average\")\n",
    "for col in df.columns:\n",
    "    if col in [\n",
    "        (\"Target 300\", \"Ratio\"),\n",
    "        (\"Target 300\", \"Ratio-small\"),\n",
    "        (\"Target 200\", \"Ratio\"),\n",
    "        (\"Target 200\", \"Ratio-small\"),\n",
    "    ]:\n",
    "        df[col] = df[col].map(lambda x: f\"{x:.1f}x\" if isinstance(x, float) else x)\n",
    "    else:\n",
    "        df[col] = df[col].map(lambda x: f\"{x:,.0f}\" if isinstance(x, int) else x)\n",
    "df.columns = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        (\n",
    "            (\"Target 300\", \"Ratio\")\n",
    "            if col == (\"Target 300\", \"Ratio-small\")\n",
    "            else (\"Target 200\", \"Ratio\") if col == (\"Target 200\", \"Ratio-small\") else col\n",
    "        )\n",
    "        for col in df.columns\n",
    "    ]\n",
    ")\n",
    "tex_table = df.to_latex(float_format=\"%.0f\", multicolumn_format=\"c|\", column_format=\"@{}l|c|cccc|cccc@{}\")\n",
    "\n",
    "tex_table = tex_table.replace(\"|}{Target 2\", \"}{Target 2\")\n",
    "tex_table = tex_table.replace(\"Average\", \"\\\\midrule Average\")\n",
    "for match in [\"Original\", \"Target 300\", \"Target 200\"]:\n",
    "    tex_table = tex_table.replace(match, f\"\\\\textbf{{{match}}}\")\n",
    "pattern = r\"\\\\midrule Average & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) & (.+) \\\\\\\\\"\n",
    "repl = r\"\\\\midrule \\\\textbf{Average} & \\\\textbf{\\1} & \\\\textbf{\\2} & \\\\textbf{\\3} & \\\\textbf{\\4} & \\\\textbf{\\5} & \\\\textbf{\\6} & \\\\textbf{\\7} & \\\\textbf{\\8} & \\\\textbf{\\9} \\\\\\\\\"\n",
    "tex_table = re.sub(pattern, repl, tex_table)\n",
    "\n",
    "with open(f\"{outdir}/lengths.tex\", \"w\") as f:\n",
    "    f.write(tex_table)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate averge ZeroScrolls summarization scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Avg summarization score for: origin - llama_3.1_70b_answer - metrics_zero_scrolls_validation.json\n",
      "21.2975\n",
      "-- Avg summarization score for: llmlingua2 - llama_3.1_70b_answer - metrics_target2000_zero_scrolls_validation.json\n",
      "17.619933333333336\n",
      "-- Avg summarization score for: llmlingua2 - llama_3.1_70b_answer - metrics_target3000_zero_scrolls_validation.json\n",
      "19.628833333333336\n",
      "-- Avg summarization score for: llmlingua2_small - llama_3.1_70b_answer - metrics_target2000_zero_scrolls_validation.json\n",
      "17.7843\n",
      "-- Avg summarization score for: llmlingua2_small - llama_3.1_70b_answer - metrics_target3000_zero_scrolls_validation.json\n",
      "19.175033333333335\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "model = [\"llama_3.1_70b_answer\", \"mistral7b_answer\", \"command_r_plus_answer\"][0]\n",
    "sum_tasks = [\"gov_report\", \"summ_screen_fd\", \"qmsum\"]\n",
    "for metrics_path in glob.glob(\"../results/zero_scrolls/**/metrics_*.json\", recursive=True):\n",
    "    if model not in metrics_path:\n",
    "        continue\n",
    "    res: dict = json.load(open(metrics_path, \"r\"))\n",
    "    avg_score = sum(res[task][\"zero_scrolls_score\"] for task in sum_tasks) / len(sum_tasks)\n",
    "    print(f'-- Avg summarization score for: {\" - \".join(metrics_path.split(\"/\")[4:])}')\n",
    "    print(avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI pricing calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "MODEL = \"gpt-4o\"\n",
    "# Normal API pricing, for batch 50% off\n",
    "pricing = {\n",
    "    \"gpt-3.5-turbo-0613\": { \"p_in\": 1.5, \"p_out\": 2.0 },\n",
    "    \"gpt-3.5-turbo\": { \"p_in\": 0.5, \"p_out\": 1.5 },\n",
    "    \"gpt-4\": { \"p_in\": 30, \"p_out\": 60 },\n",
    "    \"gpt-4-turbo\": { \"p_in\": 10, \"p_out\": 30 },\n",
    "    \"gpt-4o\": { \"p_in\": 2.5, \"p_out\": 10 },\n",
    "    \"gpt-4o-mini\": { \"p_in\": 0.15, \"p_out\": 0.6 },\n",
    "}\n",
    "get_pricing = lambda model: pricing[model]\n",
    "get_cost = lambda t_in, t_out: t_in / 1000 * get_pricing(MODEL)[\"p_in\"] / 1000 + t_out / 1000 * get_pricing(MODEL)[\"p_out\"] / 1000\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MeetingBank repro cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original QA - in: 7,050,072, out: 45,918\n",
      "Original Sum - in: 2,312,304, out: 181,927\n",
      "Compressed QA - in: 2,605,806, out: 48,353\n",
      "Compressed Sum - in: 838,737, out: 171,834\n",
      "Compressed Small QA - in: 2,390,988, out: 49,656\n",
      "Compressed Small Sum - in: 767,158, out: 168,885\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "prompt = \"Write a high-quality answer for the given question using the provided meeting transcript (which may be compressed).\\n{transcript}\\nQuestion:{question}\\nAnswer:\"\n",
    "\n",
    "in_tokens_total_mb, out_tokens_total_mb = defaultdict(int), defaultdict(int)\n",
    "\n",
    "def calc(name, d_qa, d_sum):\n",
    "    tok_in_qa = sum(len(tokenizer.encode(prompt.format(transcript=d[\"transcript\"], question=q))) for d in d_qa for q in d[\"questions\"])\n",
    "    tok_out_qa = sum(len(tokenizer.encode(a)) for d in d_qa for a in d[\"model_answers\"])\n",
    "    in_tokens_total_mb[name] += tok_in_qa\n",
    "    out_tokens_total_mb[name] += tok_out_qa\n",
    "    print(f\"{name} QA - in: {tok_in_qa:,}, out: {tok_out_qa:,}\")\n",
    "\n",
    "    tok_in_sum = sum(len(tokenizer.encode(d[\"transcript\"])) for d in d_sum)\n",
    "    tok_out_sum = sum(len(tokenizer.encode(d[\"model_summary\"])) for d in d_sum)\n",
    "    in_tokens_total_mb[name] += tok_in_sum\n",
    "    out_tokens_total_mb[name] += tok_out_sum\n",
    "    print(f\"{name} Sum - in: {tok_in_sum:,}, out: {tok_out_sum:,}\")\n",
    "\n",
    "d_orig_qa = list(json.load(open(\"../results/meetingbank_short/origin/llama_3.1_70b_answer/answer_meetingbank_QA.json\", \"r\")).values())\n",
    "d_orig_sum = list(json.load(open(\"../results/meetingbank_short/origin/llama_3.1_70b_answer/answer_meetingbank_summary.json\", \"r\")).values())\n",
    "d_comp_qa = list(json.load(open(\"../results/meetingbank_short/llmlingua2/llama_3.1_70b_answer/answer_ratio33_meetingbank_QA.json\", \"r\")).values())\n",
    "d_comp_sum = list(json.load(open(\"../results/meetingbank_short/llmlingua2/llama_3.1_70b_answer/answer_ratio33_meetingbank_summary.json\", \"r\")).values())\n",
    "d_comp_small_qa = list(json.load(open(\"../results/meetingbank_short/llmlingua2_small/llama_3.1_70b_answer/answer_ratio33_meetingbank_QA.json\", \"r\")).values())\n",
    "d_comp_small_sum = list(json.load(open(\"../results/meetingbank_short/llmlingua2_small/llama_3.1_70b_answer/answer_ratio33_meetingbank_summary.json\", \"r\")).values())\n",
    "\n",
    "calc(\"Original\", d_orig_qa, d_orig_sum)\n",
    "calc(\"Compressed\", d_comp_qa, d_comp_sum)\n",
    "calc(\"Compressed Small\", d_comp_small_qa, d_comp_small_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model: gpt-4o\n",
      "\n",
      "Total original cost: $25.68\n",
      "Total comp cost: $10.81 - savings: $14.87\n",
      "Total comp cost (small): $10.08 - savings: $15.60\n",
      "\n",
      "Total repro cost: $36.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.497617500000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_meetingbank_cost(print_res = False):\n",
    "    cost_orig = get_cost(in_tokens_total_mb[\"Original\"], out_tokens_total_mb[\"Original\"])\n",
    "    cost_comp = get_cost(in_tokens_total_mb[\"Compressed\"], out_tokens_total_mb[\"Compressed\"])\n",
    "    cost_comp_small = get_cost(in_tokens_total_mb[\"Compressed Small\"], out_tokens_total_mb[\"Compressed Small\"])\n",
    "    total_repro_cost_meetingbench = cost_comp + cost_orig\n",
    "    if print_res:\n",
    "        print(\"Target model:\", MODEL)\n",
    "        print(f\"\\nTotal original cost: ${cost_orig:.2f}\")\n",
    "        print(f\"Total comp cost: ${cost_comp:.2f} - savings: ${cost_orig - cost_comp:.2f}\")\n",
    "        print(f\"Total comp cost (small): ${cost_comp_small:.2f} - savings: ${cost_orig - cost_comp_small:.2f}\")\n",
    "        print(f\"\\nTotal repro cost: ${total_repro_cost_meetingbench:.2f}\")\n",
    "\n",
    "    return total_repro_cost_meetingbench\n",
    "\n",
    "calc_meetingbank_cost(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate LongBench repro gpt-3.5 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== origin: longbench_test_formatted ======\n",
      "Tokens in: 48,191,169 (avg: 10,146)\n",
      "Tokens out: 335,466  (avg: 71)\n",
      "Cost: $123.83\n",
      "\n",
      "====== llmlingua2: target3000_longbench_test_formatted ======\n",
      "\n",
      "Per category averages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. In Origin</th>\n",
       "      <th>Avg. In</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Saving In</th>\n",
       "      <th>Avg. Out Origin</th>\n",
       "      <th>Avg. Out</th>\n",
       "      <th>Saving Out</th>\n",
       "      <th>Cost Orig</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Saving</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Single-doc QA</th>\n",
       "      <td>11941.5</td>\n",
       "      <td>2784.5</td>\n",
       "      <td>4.3x</td>\n",
       "      <td>76.7%</td>\n",
       "      <td>12.1</td>\n",
       "      <td>10.6</td>\n",
       "      <td>12.5%</td>\n",
       "      <td>$22.48</td>\n",
       "      <td>$5.30</td>\n",
       "      <td>76.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi-doc QA</th>\n",
       "      <td>11790.2</td>\n",
       "      <td>2869.9</td>\n",
       "      <td>4.1x</td>\n",
       "      <td>75.7%</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>21.4%</td>\n",
       "      <td>$23.84</td>\n",
       "      <td>$5.94</td>\n",
       "      <td>75.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Summarization</th>\n",
       "      <td>9240.5</td>\n",
       "      <td>2436.4</td>\n",
       "      <td>3.8x</td>\n",
       "      <td>73.6%</td>\n",
       "      <td>291.7</td>\n",
       "      <td>272.5</td>\n",
       "      <td>6.6%</td>\n",
       "      <td>$20.81</td>\n",
       "      <td>$7.05</td>\n",
       "      <td>66.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Few-Shot</th>\n",
       "      <td>11004.6</td>\n",
       "      <td>2881.9</td>\n",
       "      <td>3.8x</td>\n",
       "      <td>73.8%</td>\n",
       "      <td>12.4</td>\n",
       "      <td>20.3</td>\n",
       "      <td>-63.8%</td>\n",
       "      <td>$22.11</td>\n",
       "      <td>$5.93</td>\n",
       "      <td>73.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Synthetic</th>\n",
       "      <td>10831.0</td>\n",
       "      <td>2961.0</td>\n",
       "      <td>3.7x</td>\n",
       "      <td>72.7%</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-14.0%</td>\n",
       "      <td>$16.26</td>\n",
       "      <td>$4.46</td>\n",
       "      <td>72.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <td>7108.3</td>\n",
       "      <td>3250.9</td>\n",
       "      <td>2.2x</td>\n",
       "      <td>54.3%</td>\n",
       "      <td>56.2</td>\n",
       "      <td>55.7</td>\n",
       "      <td>0.8%</td>\n",
       "      <td>$18.33</td>\n",
       "      <td>$8.68</td>\n",
       "      <td>52.6%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Avg. In Origin  Avg. In Ratio Saving In  Avg. Out Origin  \\\n",
       "Category                                                                  \n",
       "Single-doc QA         11941.5   2784.5  4.3x     76.7%             12.1   \n",
       "Multi-doc QA          11790.2   2869.9  4.1x     75.7%             32.0   \n",
       "Summarization          9240.5   2436.4  3.8x     73.6%            291.7   \n",
       "Few-Shot              11004.6   2881.9  3.8x     73.8%             12.4   \n",
       "Synthetic             10831.0   2961.0  3.7x     72.7%              2.3   \n",
       "Code                   7108.3   3250.9  2.2x     54.3%             56.2   \n",
       "\n",
       "               Avg. Out Saving Out Cost Orig   Cost Saving  \n",
       "Category                                                    \n",
       "Single-doc QA      10.6      12.5%    $22.48  $5.30  76.4%  \n",
       "Multi-doc QA       25.1      21.4%    $23.84  $5.94  75.1%  \n",
       "Summarization     272.5       6.6%    $20.81  $7.05  66.1%  \n",
       "Few-Shot           20.3     -63.8%    $22.11  $5.93  73.2%  \n",
       "Synthetic           2.7     -14.0%    $16.26  $4.46  72.6%  \n",
       "Code               55.7       0.8%    $18.33  $8.68  52.6%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in: 13,666,330 (avg: 2,877) - saving: 71.64%\n",
      "Tokens out: 319,548  (avg: 67) - saving: 4.75%\n",
      "Cost: $37.36 - saving: 69.83% \n",
      "\n",
      "Per task averages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Avg. In Origin</th>\n",
       "      <th>Avg. In</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Saving In</th>\n",
       "      <th>Avg. Out Origin</th>\n",
       "      <th>Avg. Out</th>\n",
       "      <th>Saving Out</th>\n",
       "      <th>Cost Orig</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Saving</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Single-doc QA</th>\n",
       "      <th>MultiFieldQA (EN)</th>\n",
       "      <td>6877.1</td>\n",
       "      <td>2914.9</td>\n",
       "      <td>2.4x</td>\n",
       "      <td>57.6%</td>\n",
       "      <td>21.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.2%</td>\n",
       "      <td>$2.61</td>\n",
       "      <td>$1.12</td>\n",
       "      <td>57.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiFieldQA (ZH)</th>\n",
       "      <td>5010.1</td>\n",
       "      <td>1968.6</td>\n",
       "      <td>2.5x</td>\n",
       "      <td>60.7%</td>\n",
       "      <td>15.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>12.9%</td>\n",
       "      <td>$2.54</td>\n",
       "      <td>$1.01</td>\n",
       "      <td>60.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NarrativeQA</th>\n",
       "      <td>29541.4</td>\n",
       "      <td>3149.5</td>\n",
       "      <td>9.4x</td>\n",
       "      <td>89.3%</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>17.4%</td>\n",
       "      <td>$14.78</td>\n",
       "      <td>$1.58</td>\n",
       "      <td>89.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qasper</th>\n",
       "      <td>5071.2</td>\n",
       "      <td>3137.4</td>\n",
       "      <td>1.6x</td>\n",
       "      <td>38.1%</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.3%</td>\n",
       "      <td>$2.55</td>\n",
       "      <td>$1.58</td>\n",
       "      <td>38.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>11941.5</td>\n",
       "      <td>2784.5</td>\n",
       "      <td>4.3x</td>\n",
       "      <td>76.7%</td>\n",
       "      <td>12.1</td>\n",
       "      <td>10.6</td>\n",
       "      <td>12.5%</td>\n",
       "      <td>$22.48</td>\n",
       "      <td>$5.30</td>\n",
       "      <td>76.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Multi-doc QA</th>\n",
       "      <th>HotpotQA</th>\n",
       "      <td>12690.7</td>\n",
       "      <td>3256.9</td>\n",
       "      <td>3.9x</td>\n",
       "      <td>74.3%</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>17.9%</td>\n",
       "      <td>$6.35</td>\n",
       "      <td>$1.64</td>\n",
       "      <td>74.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2WikiMultihopQA</th>\n",
       "      <td>7033.4</td>\n",
       "      <td>3029.8</td>\n",
       "      <td>2.3x</td>\n",
       "      <td>56.9%</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>9.1%</td>\n",
       "      <td>$3.53</td>\n",
       "      <td>$1.52</td>\n",
       "      <td>56.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuSiQue</th>\n",
       "      <td>15430.1</td>\n",
       "      <td>3327.3</td>\n",
       "      <td>4.6x</td>\n",
       "      <td>78.4%</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>24.1%</td>\n",
       "      <td>$7.73</td>\n",
       "      <td>$1.67</td>\n",
       "      <td>78.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DuReader (ZH)</th>\n",
       "      <td>12006.5</td>\n",
       "      <td>1865.7</td>\n",
       "      <td>6.4x</td>\n",
       "      <td>84.5%</td>\n",
       "      <td>112.5</td>\n",
       "      <td>87.8</td>\n",
       "      <td>21.9%</td>\n",
       "      <td>$6.23</td>\n",
       "      <td>$1.11</td>\n",
       "      <td>82.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>11790.2</td>\n",
       "      <td>2869.9</td>\n",
       "      <td>4.1x</td>\n",
       "      <td>75.7%</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>21.4%</td>\n",
       "      <td>$23.84</td>\n",
       "      <td>$5.94</td>\n",
       "      <td>75.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Summarization</th>\n",
       "      <th>GovReport</th>\n",
       "      <td>10246.1</td>\n",
       "      <td>3115.2</td>\n",
       "      <td>3.3x</td>\n",
       "      <td>69.6%</td>\n",
       "      <td>333.5</td>\n",
       "      <td>364.2</td>\n",
       "      <td>-9.2%</td>\n",
       "      <td>$5.79</td>\n",
       "      <td>$2.29</td>\n",
       "      <td>60.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QMSum</th>\n",
       "      <td>13502.9</td>\n",
       "      <td>2855.3</td>\n",
       "      <td>4.7x</td>\n",
       "      <td>78.9%</td>\n",
       "      <td>119.9</td>\n",
       "      <td>108.5</td>\n",
       "      <td>9.5%</td>\n",
       "      <td>$6.99</td>\n",
       "      <td>$1.64</td>\n",
       "      <td>76.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNews</th>\n",
       "      <td>2623.0</td>\n",
       "      <td>2037.5</td>\n",
       "      <td>1.3x</td>\n",
       "      <td>22.3%</td>\n",
       "      <td>362.2</td>\n",
       "      <td>364.2</td>\n",
       "      <td>-0.5%</td>\n",
       "      <td>$2.04</td>\n",
       "      <td>$1.75</td>\n",
       "      <td>14.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VCSUM (ZH)</th>\n",
       "      <td>10589.9</td>\n",
       "      <td>1737.3</td>\n",
       "      <td>6.1x</td>\n",
       "      <td>83.6%</td>\n",
       "      <td>351.2</td>\n",
       "      <td>253.2</td>\n",
       "      <td>27.9%</td>\n",
       "      <td>$6.00</td>\n",
       "      <td>$1.38</td>\n",
       "      <td>77.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>9240.5</td>\n",
       "      <td>2436.4</td>\n",
       "      <td>3.8x</td>\n",
       "      <td>73.6%</td>\n",
       "      <td>291.7</td>\n",
       "      <td>272.5</td>\n",
       "      <td>6.6%</td>\n",
       "      <td>$20.81</td>\n",
       "      <td>$7.05</td>\n",
       "      <td>66.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Few-Shot</th>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>11602.2</td>\n",
       "      <td>3721.2</td>\n",
       "      <td>3.1x</td>\n",
       "      <td>67.9%</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.6%</td>\n",
       "      <td>$5.81</td>\n",
       "      <td>$1.87</td>\n",
       "      <td>67.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAMSum</th>\n",
       "      <td>8834.2</td>\n",
       "      <td>2893.5</td>\n",
       "      <td>3.1x</td>\n",
       "      <td>67.2%</td>\n",
       "      <td>39.9</td>\n",
       "      <td>49.8</td>\n",
       "      <td>-24.9%</td>\n",
       "      <td>$4.50</td>\n",
       "      <td>$1.55</td>\n",
       "      <td>65.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TREC</th>\n",
       "      <td>6745.8</td>\n",
       "      <td>2674.3</td>\n",
       "      <td>2.5x</td>\n",
       "      <td>60.4%</td>\n",
       "      <td>2.4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-560.5%</td>\n",
       "      <td>$3.38</td>\n",
       "      <td>$1.37</td>\n",
       "      <td>59.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSHT (ZH)</th>\n",
       "      <td>16836.2</td>\n",
       "      <td>2238.7</td>\n",
       "      <td>7.5x</td>\n",
       "      <td>86.7%</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12.3</td>\n",
       "      <td>-186.3%</td>\n",
       "      <td>$8.43</td>\n",
       "      <td>$1.14</td>\n",
       "      <td>86.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>11004.6</td>\n",
       "      <td>2881.9</td>\n",
       "      <td>3.8x</td>\n",
       "      <td>73.8%</td>\n",
       "      <td>12.4</td>\n",
       "      <td>20.3</td>\n",
       "      <td>-63.8%</td>\n",
       "      <td>$22.11</td>\n",
       "      <td>$5.93</td>\n",
       "      <td>73.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Synthetic</th>\n",
       "      <th>PassageCount</th>\n",
       "      <td>14798.8</td>\n",
       "      <td>3328.3</td>\n",
       "      <td>4.4x</td>\n",
       "      <td>77.5%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>$7.40</td>\n",
       "      <td>$1.67</td>\n",
       "      <td>77.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassageRetrieval (EN)</th>\n",
       "      <td>12374.2</td>\n",
       "      <td>3439.8</td>\n",
       "      <td>3.6x</td>\n",
       "      <td>72.2%</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>$6.19</td>\n",
       "      <td>$1.73</td>\n",
       "      <td>72.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassageRetrieval (ZH)</th>\n",
       "      <td>5319.9</td>\n",
       "      <td>2114.9</td>\n",
       "      <td>2.5x</td>\n",
       "      <td>60.2%</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-32.7%</td>\n",
       "      <td>$2.67</td>\n",
       "      <td>$1.07</td>\n",
       "      <td>60.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>10831.0</td>\n",
       "      <td>2961.0</td>\n",
       "      <td>3.7x</td>\n",
       "      <td>72.7%</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-14.0%</td>\n",
       "      <td>$16.26</td>\n",
       "      <td>$4.46</td>\n",
       "      <td>72.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Code</th>\n",
       "      <th>LCC</th>\n",
       "      <td>3218.1</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.4x</td>\n",
       "      <td>28.3%</td>\n",
       "      <td>57.9</td>\n",
       "      <td>55.3</td>\n",
       "      <td>4.6%</td>\n",
       "      <td>$4.31</td>\n",
       "      <td>$3.16</td>\n",
       "      <td>26.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RepoBench-P</th>\n",
       "      <td>10998.5</td>\n",
       "      <td>4195.7</td>\n",
       "      <td>2.6x</td>\n",
       "      <td>61.9%</td>\n",
       "      <td>54.4</td>\n",
       "      <td>56.2</td>\n",
       "      <td>-3.2%</td>\n",
       "      <td>$14.02</td>\n",
       "      <td>$5.53</td>\n",
       "      <td>60.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>7108.3</td>\n",
       "      <td>3250.9</td>\n",
       "      <td>2.2x</td>\n",
       "      <td>54.3%</td>\n",
       "      <td>56.2</td>\n",
       "      <td>55.7</td>\n",
       "      <td>0.8%</td>\n",
       "      <td>$18.33</td>\n",
       "      <td>$8.68</td>\n",
       "      <td>52.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <th>Total</th>\n",
       "      <td>10145.5</td>\n",
       "      <td>2877.1</td>\n",
       "      <td>3.5x</td>\n",
       "      <td>71.6%</td>\n",
       "      <td>70.6</td>\n",
       "      <td>67.3</td>\n",
       "      <td>4.7%</td>\n",
       "      <td>$123.83</td>\n",
       "      <td>$37.36</td>\n",
       "      <td>69.8%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Avg. In Origin  Avg. In Ratio Saving In  \\\n",
       "Category      Task                                                             \n",
       "Single-doc QA MultiFieldQA (EN)              6877.1   2914.9  2.4x     57.6%   \n",
       "              MultiFieldQA (ZH)              5010.1   1968.6  2.5x     60.7%   \n",
       "              NarrativeQA                   29541.4   3149.5  9.4x     89.3%   \n",
       "              Qasper                         5071.2   3137.4  1.6x     38.1%   \n",
       "              Average                       11941.5   2784.5  4.3x     76.7%   \n",
       "Multi-doc QA  HotpotQA                      12690.7   3256.9  3.9x     74.3%   \n",
       "              2WikiMultihopQA                7033.4   3029.8  2.3x     56.9%   \n",
       "              MuSiQue                       15430.1   3327.3  4.6x     78.4%   \n",
       "              DuReader (ZH)                 12006.5   1865.7  6.4x     84.5%   \n",
       "              Average                       11790.2   2869.9  4.1x     75.7%   \n",
       "Summarization GovReport                     10246.1   3115.2  3.3x     69.6%   \n",
       "              QMSum                         13502.9   2855.3  4.7x     78.9%   \n",
       "              MultiNews                      2623.0   2037.5  1.3x     22.3%   \n",
       "              VCSUM (ZH)                    10589.9   1737.3  6.1x     83.6%   \n",
       "              Average                        9240.5   2436.4  3.8x     73.6%   \n",
       "Few-Shot      TriviaQA                      11602.2   3721.2  3.1x     67.9%   \n",
       "              SAMSum                         8834.2   2893.5  3.1x     67.2%   \n",
       "              TREC                           6745.8   2674.3  2.5x     60.4%   \n",
       "              LSHT (ZH)                     16836.2   2238.7  7.5x     86.7%   \n",
       "              Average                       11004.6   2881.9  3.8x     73.8%   \n",
       "Synthetic     PassageCount                  14798.8   3328.3  4.4x     77.5%   \n",
       "              PassageRetrieval (EN)         12374.2   3439.8  3.6x     72.2%   \n",
       "              PassageRetrieval (ZH)          5319.9   2114.9  2.5x     60.2%   \n",
       "              Average                       10831.0   2961.0  3.7x     72.7%   \n",
       "Code          LCC                            3218.1   2306.0  1.4x     28.3%   \n",
       "              RepoBench-P                   10998.5   4195.7  2.6x     61.9%   \n",
       "              Average                        7108.3   3250.9  2.2x     54.3%   \n",
       "Total         Total                         10145.5   2877.1  3.5x     71.6%   \n",
       "\n",
       "                                     Avg. Out Origin  Avg. Out Saving Out  \\\n",
       "Category      Task                                                          \n",
       "Single-doc QA MultiFieldQA (EN)                 21.2      19.0      10.2%   \n",
       "              MultiFieldQA (ZH)                 15.6      13.6      12.9%   \n",
       "              NarrativeQA                        4.5       3.7      17.4%   \n",
       "              Qasper                             9.3       8.0      13.3%   \n",
       "              Average                           12.1      10.6      12.5%   \n",
       "Multi-doc QA  HotpotQA                           4.5       3.7      17.9%   \n",
       "              2WikiMultihopQA                    4.8       4.4       9.1%   \n",
       "              MuSiQue                            6.0       4.6      24.1%   \n",
       "              DuReader (ZH)                    112.5      87.8      21.9%   \n",
       "              Average                           32.0      25.1      21.4%   \n",
       "Summarization GovReport                        333.5     364.2      -9.2%   \n",
       "              QMSum                            119.9     108.5       9.5%   \n",
       "              MultiNews                        362.2     364.2      -0.5%   \n",
       "              VCSUM (ZH)                       351.2     253.2      27.9%   \n",
       "              Average                          291.7     272.5       6.6%   \n",
       "Few-Shot      TriviaQA                           2.9       3.0      -2.6%   \n",
       "              SAMSum                            39.9      49.8     -24.9%   \n",
       "              TREC                               2.4      16.0    -560.5%   \n",
       "              LSHT (ZH)                          4.3      12.3    -186.3%   \n",
       "              Average                           12.4      20.3     -63.8%   \n",
       "Synthetic     PassageCount                       1.0       1.0       0.0%   \n",
       "              PassageRetrieval (EN)              3.0       3.0       0.0%   \n",
       "              PassageRetrieval (ZH)              3.0       4.0     -32.7%   \n",
       "              Average                            2.3       2.7     -14.0%   \n",
       "Code          LCC                               57.9      55.3       4.6%   \n",
       "              RepoBench-P                       54.4      56.2      -3.2%   \n",
       "              Average                           56.2      55.7       0.8%   \n",
       "Total         Total                             70.6      67.3       4.7%   \n",
       "\n",
       "                                    Cost Orig    Cost Saving  \n",
       "Category      Task                                            \n",
       "Single-doc QA MultiFieldQA (EN)         $2.61   $1.12  57.0%  \n",
       "              MultiFieldQA (ZH)         $2.54   $1.01  60.1%  \n",
       "              NarrativeQA              $14.78   $1.58  89.3%  \n",
       "              Qasper                    $2.55   $1.58  38.0%  \n",
       "              Average                  $22.48   $5.30  76.4%  \n",
       "Multi-doc QA  HotpotQA                  $6.35   $1.64  74.3%  \n",
       "              2WikiMultihopQA           $3.53   $1.52  56.8%  \n",
       "              MuSiQue                   $7.73   $1.67  78.4%  \n",
       "              DuReader (ZH)             $6.23   $1.11  82.2%  \n",
       "              Average                  $23.84   $5.94  75.1%  \n",
       "Summarization GovReport                 $5.79   $2.29  60.5%  \n",
       "              QMSum                     $6.99   $1.64  76.5%  \n",
       "              MultiNews                 $2.04   $1.75  14.2%  \n",
       "              VCSUM (ZH)                $6.00   $1.38  77.1%  \n",
       "              Average                  $20.81   $7.05  66.1%  \n",
       "Few-Shot      TriviaQA                  $5.81   $1.87  67.9%  \n",
       "              SAMSum                    $4.50   $1.55  65.6%  \n",
       "              TREC                      $3.38   $1.37  59.5%  \n",
       "              LSHT (ZH)                 $8.43   $1.14  86.4%  \n",
       "              Average                  $22.11   $5.93  73.2%  \n",
       "Synthetic     PassageCount              $7.40   $1.67  77.5%  \n",
       "              PassageRetrieval (EN)     $6.19   $1.73  72.1%  \n",
       "              PassageRetrieval (ZH)     $2.67   $1.07  60.0%  \n",
       "              Average                  $16.26   $4.46  72.6%  \n",
       "Code          LCC                       $4.31   $3.16  26.7%  \n",
       "              RepoBench-P              $14.02   $5.53  60.6%  \n",
       "              Average                  $18.33   $8.68  52.6%  \n",
       "Total         Total                   $123.83  $37.36  69.8%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Depends on build_prompt_longbench (see above)\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "target_model = \"gpt_4o_answer\"\n",
    "data_files = {\n",
    "    \"origin\": \"../results/longbench/origin/longbench_test_formatted.json\",\n",
    "    \"llmlingua2_2000\": \"../results/longbench/llmlingua2/compression_target2000_longbench_test_formatted.json\",\n",
    "    \"llmlingua2_3000\": \"../results/longbench/llmlingua2/compression_target3000_longbench_test_formatted.json\",\n",
    "    \"llmlingua2_small_2000\": \"../results/longbench/llmlingua2_small/compression_target2000_longbench_test_formatted.json\",\n",
    "    \"llmlingua2_small_2000\": \"../results/longbench/llmlingua2_small/compression_target3000_longbench_test_formatted.json\",\n",
    "    \"zero_shot\": \"../results/longbench/zero_shot/longbench_test_formatted.json\",\n",
    "}\n",
    "categories = {\n",
    "    \"Single-doc QA\": [\"multifieldqa_en\", \"multifieldqa_zh\", \"narrativeqa\", \"qasper\"],\n",
    "    \"Multi-doc QA\": [\"hotpotqa\", \"2wikimqa\", \"musique\", \"dureader\"],\n",
    "    \"Summarization\": [\"gov_report\", \"qmsum\", \"multi_news\", \"vcsum\"],\n",
    "    \"Few-Shot\": [\"triviaqa\", \"samsum\", \"trec\", \"lsht\"],\n",
    "    \"Synthetic\": [\"passage_count\", \"passage_retrieval_en\", \"passage_retrieval_zh\"],\n",
    "    \"Code\": [\"lcc\", \"repobench-p\"],\n",
    "}\n",
    "reverse_categories = {v: k for k, values in categories.items() for v in values}\n",
    "reverse_categories[\"total\"] = \"Total\"\n",
    "tasknames = {\n",
    "    \"narrativeqa\": \"NarrativeQA\",\n",
    "    \"qasper\": \"Qasper\",\n",
    "    \"multifieldqa_en\": \"MultiFieldQA (EN)\",\n",
    "    \"multifieldqa_zh\": \"MultiFieldQA (ZH)\",\n",
    "    \"hotpotqa\": \"HotpotQA\",\n",
    "    \"2wikimqa\": \"2WikiMultihopQA\",\n",
    "    \"musique\": \"MuSiQue\",\n",
    "    \"dureader\": \"DuReader (ZH)\",\n",
    "    \"gov_report\": \"GovReport\",\n",
    "    \"qmsum\": \"QMSum\",\n",
    "    \"multi_news\": \"MultiNews\",\n",
    "    \"vcsum\": \"VCSUM (ZH)\",\n",
    "    \"trec\": \"TREC\",\n",
    "    \"triviaqa\": \"TriviaQA\",\n",
    "    \"samsum\": \"SAMSum\",\n",
    "    \"lsht\": \"LSHT (ZH)\",\n",
    "    \"passage_retrieval_en\": \"PassageRetrieval (EN)\",\n",
    "    \"passage_count\": \"PassageCount\",\n",
    "    \"passage_retrieval_zh\": \"PassageRetrieval (ZH)\",\n",
    "    \"lcc\": \"LCC\",\n",
    "    \"repobench-p\": \"RepoBench-P\",\n",
    "    \"Average\": \"Average\",\n",
    "    \"total\": \"Total\",\n",
    "}\n",
    "col_order = [\n",
    "    \"Avg. In Origin\",\n",
    "    \"Avg. In\",\n",
    "    \"Ratio\",\n",
    "    \"Saving In\",\n",
    "    \"Avg. Out Origin\",\n",
    "    \"Avg. Out\",\n",
    "    \"Saving Out\",\n",
    "    \"Cost Orig\",\n",
    "    \"Cost\",\n",
    "    \"Saving\",\n",
    "]\n",
    "\n",
    "in_tokens_total_lb, out_tokens_total_lb = defaultdict(int), defaultdict(int)\n",
    "scenario_data = {scenario: {} for scenario in data_files.keys()}\n",
    "for scenario, data_file in data_files.items():\n",
    "    get_split_name = lambda path: list(map(lambda part: part.replace(\".json\", \"\"), path.split(\"/\")))\n",
    "    filename_split = get_split_name(data_file)\n",
    "    model, target = filename_split[-2], filename_split[-1].replace(\"compression_\", \"\")\n",
    "    file_dir = os.path.dirname(data_file)\n",
    "    answer_file = f\"{file_dir}/{target_model}/answer_{target}.json\"\n",
    "    if not os.path.exists(data_file) or not os.path.exists(answer_file):\n",
    "        continue\n",
    "    print(f\"\\n====== {model}: {target} ======\")\n",
    "    longbench_data = json.load(open(data_file, \"r\"))\n",
    "    if isinstance(longbench_data, dict):\n",
    "        longbench_data = list(longbench_data.values())\n",
    "    compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "    load_key = None if model == \"zero_shot\" else \"compressed_prompt\" if compression else \"context\"\n",
    "    in_tokens = defaultdict(int)\n",
    "    for sample in longbench_data:\n",
    "        prompt = build_prompt_longbench(sample, load_key)\n",
    "        in_tokens[sample[\"task\"]] += len(tokenizer.encode(prompt))\n",
    "    in_tokens_total = sum(in_tokens.values())\n",
    "    in_tokens[\"total\"] = in_tokens_total\n",
    "    scenario_data[scenario][\"in_tokens\"] = in_tokens\n",
    "    in_tokens_total_lb[model] += in_tokens_total\n",
    "    res_data = list(json.load(open(answer_file, \"r\")).values())\n",
    "    out_tokens = defaultdict(int)\n",
    "    num_samples = defaultdict(int)\n",
    "    num_samples[\"total\"] = len(res_data)\n",
    "    for d in res_data:\n",
    "        out_tokens[d[\"task\"]] += len(tokenizer.encode(d[\"pred\"]))\n",
    "        num_samples[d[\"task\"]] += 1\n",
    "    out_tokens_total = sum(out_tokens.values())\n",
    "    out_tokens[\"total\"] = out_tokens_total\n",
    "    scenario_data[scenario][\"out_tokens\"] = out_tokens\n",
    "    out_tokens_total_lb[model] += out_tokens_total\n",
    "\n",
    "    if scenario == \"origin\":\n",
    "        total_cost_origin = get_cost(in_tokens_total, out_tokens_total)\n",
    "        print(\n",
    "            f\"Tokens in: {in_tokens_total:,} (avg: {in_tokens_total / len(longbench_data):,.0f})\\n\"\n",
    "            f\"Tokens out: {out_tokens_total:,}  (avg: {out_tokens_total / len(longbench_data):,.0f})\\n\"\n",
    "            f\"Cost: ${total_cost_origin:.2f}\"\n",
    "        )\n",
    "    else:\n",
    "        total_cost = get_cost(in_tokens_total, out_tokens_total)\n",
    "        total_tokens = f\"Tokens in: {in_tokens_total:,} (avg: {in_tokens_total / len(longbench_data):,.0f}) - saving: {(1 - in_tokens_total / scenario_data['origin']['in_tokens']['total']) * 100:.2f}%\\n\"\n",
    "        total_tokens += f\"Tokens out: {out_tokens_total:,}  (avg: {out_tokens_total / len(longbench_data):,.0f}) - saving: {(1 - out_tokens_total / scenario_data['origin']['out_tokens']['total']) * 100:.2f}%\\n\"\n",
    "        total_tokens += f\"Cost: ${total_cost:.2f} - saving: {(1 - total_cost / total_cost_origin) * 100:.2f}%\"\n",
    "\n",
    "        print(\"\\nPer category averages:\")\n",
    "        per_cat = defaultdict(list)\n",
    "        for category, tasks in categories.items():\n",
    "            in_t = sum(in_tokens[task] for task in tasks)\n",
    "            out_t = sum(out_tokens[task] for task in tasks)\n",
    "            avg_in = in_t / sum(num_samples[task] for task in tasks)\n",
    "            avg_out = out_t / sum(num_samples[task] for task in tasks)\n",
    "            cost = get_cost(in_t, out_t)\n",
    "            per_cat[\"Category\"].append(category)\n",
    "            per_cat[\"Avg. In\"].append(round(avg_in, 1))\n",
    "            per_cat[\"Avg. Out\"].append(round(avg_out, 1))\n",
    "            in_t_origin = sum(scenario_data[\"origin\"][\"in_tokens\"][task] for task in tasks)\n",
    "            out_t_origin = sum(scenario_data[\"origin\"][\"out_tokens\"][task] for task in tasks)\n",
    "            cost_orig = get_cost(in_t_origin, out_t_origin)\n",
    "            avg_in_origin = in_t_origin / sum(num_samples[task] for task in tasks)\n",
    "            per_cat[\"Avg. In Origin\"].append(round(avg_in_origin, 1))\n",
    "            per_cat[\"Ratio\"].append(f\"{avg_in_origin / avg_in:.1f}x\")\n",
    "            per_cat[\"Avg. Out Origin\"].append(round(out_t_origin / sum(num_samples[task] for task in tasks), 1))\n",
    "            per_cat[\"Saving In\"].append(f\"{(1 - in_t / in_t_origin) * 100:.1f}%\")\n",
    "            per_cat[\"Saving Out\"].append(f\"{(1 - out_t / out_t_origin) * 100:.1f}%\")\n",
    "            per_cat[\"Cost Orig\"].append(f\"${cost_orig:.2f}\")\n",
    "            per_cat[\"Cost\"].append(f\"${cost:.2f}\")\n",
    "            per_cat[\"Saving\"].append(f\"{(1 - cost / cost_orig) * 100:.1f}%\")\n",
    "        per_cat = pd.DataFrame(per_cat)\n",
    "        per_cat.set_index(\"Category\", inplace=True)\n",
    "        per_cat = per_cat[col_order]\n",
    "        display(per_cat)\n",
    "\n",
    "        print(total_tokens, \"\\n\\nPer task averages:\")\n",
    "        per_task = defaultdict(list)\n",
    "        for task, in_t in in_tokens.items():\n",
    "            avg_in = in_t / num_samples[task]\n",
    "            avg_out = out_tokens[task] / num_samples[task]\n",
    "            cost = get_cost(in_t, out_tokens[task])\n",
    "            per_task[\"Category\"].append(reverse_categories[task])\n",
    "            per_task[\"Task\"].append(tasknames[task])\n",
    "            per_task[\"Avg. In\"].append(round(avg_in, 1))\n",
    "            per_task[\"Avg. Out\"].append(round(avg_out, 1))\n",
    "            in_t_origin, out_t_origin = (\n",
    "                scenario_data[\"origin\"][\"in_tokens\"][task],\n",
    "                scenario_data[\"origin\"][\"out_tokens\"][task],\n",
    "            )\n",
    "            cost_orig = get_cost(in_t_origin, out_t_origin)\n",
    "            avg_in_origin = in_t_origin / num_samples[task]\n",
    "            per_task[\"Avg. In Origin\"].append(round(avg_in_origin, 1))\n",
    "            per_task[\"Ratio\"].append(f\"{avg_in_origin / avg_in:.1f}x\")\n",
    "            per_task[\"Avg. Out Origin\"].append(round(out_t_origin / num_samples[task], 1))\n",
    "            per_task[\"Saving In\"].append(f\"{(1 - in_t / in_t_origin) * 100:.1f}%\")\n",
    "            per_task[\"Saving Out\"].append(f\"{(1 - out_tokens[task] / out_t_origin) * 100:.1f}%\")\n",
    "            per_task[\"Cost Orig\"].append(f\"${cost_orig:.2f}\")\n",
    "            per_task[\"Cost\"].append(f\"${cost:.2f}\")\n",
    "            per_task[\"Saving\"].append(f\"{(1 - cost / cost_orig) * 100:.1f}%\")\n",
    "        per_task = pd.DataFrame(per_task)\n",
    "        per_task.set_index(\"Category\", inplace=True)\n",
    "        per_task = per_task.set_index(\"Task\", append=True)\n",
    "        per_task = per_task[col_order]\n",
    "        per_cat[\"Task\"] = \"Average\"\n",
    "        per_cat = per_cat.set_index(\"Task\", append=True)\n",
    "        per_task = (\n",
    "            pd.concat([per_task, per_cat])\n",
    "            .sort_index(\n",
    "                level=1,\n",
    "                key=lambda col: col.map(\n",
    "                    {\n",
    "                        val: i\n",
    "                        for i, val in enumerate([*[tasknames[t] for c in categories.values() for t in c], \"Average\"])\n",
    "                    }\n",
    "                ),\n",
    "            )\n",
    "            .sort_index(\n",
    "                level=[0, 1],\n",
    "                key=lambda col: col.map({val: i for i, val in enumerate([*list(categories.keys()), \"Total\"])}),\n",
    "            )\n",
    "        )\n",
    "        display(per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "out_dir = \"../../ma-thesis/tables\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "cols = [\n",
    "    [\"\\\\textbf{Input}\", \"$\\\\varnothing$ Size\"],\n",
    "    [\"\\\\textbf{Input}\", \"$\\\\varnothing$ Comp.\"],\n",
    "    [\"\\\\textbf{Input}\", \"Ratio\"],\n",
    "    [\"\\\\textbf{Input}\", \"Saving\"],\n",
    "    [\"\\\\textbf{Output}\", \"$\\\\varnothing$ Size\"],\n",
    "    [\"\\\\textbf{Output}\", \"$\\\\varnothing$ Comp.\"],\n",
    "    [\"\\\\textbf{Output}\", \"Saving\"],\n",
    "    [\"\\\\textbf{Cost}\", \"Original\"],\n",
    "    [\"\\\\textbf{Cost}\", \"Comp.\"],\n",
    "    [\"\\\\textbf{Cost}\", \"Saving\"],\n",
    "]\n",
    "for df, filename in zip([per_task, per_cat], [\"longbench_savings_per_task\", \"longbench_savings_per_category\"]):\n",
    "    df.columns = pd.MultiIndex.from_tuples(cols)\n",
    "    df = df.map(lambda x: x.replace(\"%\", r\"\\%\").replace(\"$\", \"\\$\") if isinstance(x, str) else f\"{x:,}\")\n",
    "    df = df.apply(\n",
    "        lambda row: (row if row.name[1] not in [\"Average\", \"Total\"] else row.apply(lambda x: f\"\\\\textbf{{{x}}}\")),\n",
    "        axis=1,\n",
    "    )\n",
    "    tex_table = df.to_latex(\n",
    "        float_format=\"%.1f\",\n",
    "        multicolumn_format=\"c|\",\n",
    "        column_format=\"@{}l@{\\hskip 2pt}|rrrr|rrr|rrr@{}\",\n",
    "        index_names=False,\n",
    "    )\n",
    "    tex_table = (\n",
    "        tex_table.replace(\"\\\\cline{1-12}\", \"\\\\midrule\", 6)\n",
    "        .replace(\"\\\\cline{1-12}\", \"\")\n",
    "        .replace(\"{c|}{\\\\textbf{C\", \"{c}{\\\\textbf{C\")\n",
    "        # .replace(\"-doc\", \"-d.\")\n",
    "        # .replace(\"Summarization\", \"Summariz.\")\n",
    "        # .replace(\"Total &\", \"\\\\multicolumn{2}{@{}l}{\\\\textbf{Total}}\")\n",
    "    )\n",
    "    for match in [\"Average\", \"Total\"]:\n",
    "        tex_table = tex_table.replace(match, f\"\\\\textbf{{{match}}}\")\n",
    "    # tex_table = re.sub(\n",
    "    #     r\"\\\\multirow\\[t\\]{(\\d+)}{.*}{(.*)}\",\n",
    "    #     r\"\\\\parbox[t]{2mm}{\\\\multirow{\\1}{*}{\\\\rotatebox[origin=c]{90}{\\\\textbf{\\2}}}}\",\n",
    "    #     tex_table,\n",
    "    # )\n",
    "    for category in categories:\n",
    "        tex_table = tex_table.replace(\"Average\", category, 1)\n",
    "    tex_table = re.sub(r\"^.*?&\", \"\", tex_table, flags=re.M)  # Remove category column\n",
    "    with open(f\"{out_dir}/{filename}.tex\", \"w\") as f:\n",
    "        f.write(tex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_longbench_cost(print_res = False):\n",
    "    cost_origin = get_cost(in_tokens_total_lb[\"origin\"], out_tokens_total_lb[\"origin\"])\n",
    "    cost_comp = get_cost(in_tokens_total_lb[\"llmlingua2\"], out_tokens_total_lb[\"llmlingua2\"])\n",
    "    cost_comp_small = get_cost(in_tokens_total_lb[\"llmlingua2_small\"], out_tokens_total_lb[\"llmlingua2_small\"])\n",
    "    cost_zero_shot = get_cost(in_tokens_total_lb[\"zero_shot\"], out_tokens_total_lb[\"zero_shot\"])\n",
    "\n",
    "    total_repro_cost_longbench = cost_comp + cost_origin\n",
    "    cost_comp_3k = get_cost(14649554, 318541)\n",
    "    if print_res:\n",
    "        print(\"Target model:\", MODEL)\n",
    "        print(f\"\\nTotal original cost: ${cost_origin:.2f}\")\n",
    "        print(f\"Total comp cost: ${cost_comp:.2f} - savings: ${cost_origin - cost_comp:.2f}\")\n",
    "        print(f\"Total comp cost (small): ${cost_comp_small:.2f} - savings: ${cost_origin - cost_comp_small:.2f}\")\n",
    "        print(f\"Total zero-shot cost: ${cost_zero_shot:.2f} - savings: ${cost_origin - cost_zero_shot:.2f}\")\n",
    "        print(f\"\\nTotal repro cost: ${total_repro_cost_longbench:.2f}\")\n",
    "        print(f\"Total comp cost (3k): ${cost_comp_3k:.2f} - savings: ${cost_origin - cost_comp_3k:.2f}\")\n",
    "    \n",
    "    return total_repro_cost_longbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model: gpt-4o\n",
      "\n",
      "Total original cost: $123.78\n",
      "Total comp cost: $37.08 - savings: $86.70\n",
      "Total comp cost (small): $72.18 - savings: $51.60\n",
      "Total zero-shot cost: $3.79 - savings: $119.99\n",
      "\n",
      "Total repro cost: $160.86\n",
      "Total comp cost (3k): $39.81 - savings: $83.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160.85512749999998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_longbench_cost(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ZeroScrolls repro gpt-3.5 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== origin: zero_scrolls_validation ======\n",
      "Tokens in: 2,593,614\n",
      "Tokens out: 34,106\n",
      "\n",
      "====== llmlingua2: target3000_zero_scrolls_validation ======\n",
      "Tokens in: 797,605\n",
      "Tokens out: 30,781\n",
      "\n",
      "====== llmlingua2_small: target2000_zero_scrolls_validation ======\n",
      "Tokens in: 562,490\n",
      "Tokens out: 28,921\n",
      "\n",
      "====== llmlingua2_small: target3000_zero_scrolls_validation ======\n",
      "Tokens in: 802,676\n",
      "Tokens out: 31,674\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "target_model = \"llama_3.1_70b_answer\"\n",
    "data_files = [\n",
    "    \"../results/zero_scrolls/origin/zero_scrolls_validation.json\",\n",
    "    # \"../results/zero_scrolls/llmlingua2/compression_target2000_zero_scrolls_validation.json\",\n",
    "    \"../results/zero_scrolls/llmlingua2/compression_target3000_zero_scrolls_validation.json\",\n",
    "    \"../results/zero_scrolls/llmlingua2_small/compression_target2000_zero_scrolls_validation.json\",\n",
    "    \"../results/zero_scrolls/llmlingua2_small/compression_target3000_zero_scrolls_validation.json\",\n",
    "    # \"../results/zero_scrolls/zero_shot/zero_scrolls_validation.json\",\n",
    "]\n",
    "in_tokens_total_zs, out_tokens_total_zs = defaultdict(int), defaultdict(int)\n",
    "for data_file in data_files:\n",
    "    get_split_name = lambda path: list(map(lambda part: part.replace(\".json\", \"\"), path.split(\"/\")))\n",
    "    filename_split = get_split_name(data_file)\n",
    "    model, target = filename_split[-2], filename_split[-1].replace(\"compression_\", \"\")\n",
    "    print(f\"\\n====== {model}: {target} ======\")\n",
    "    zero_scrolls_data = json.load(open(data_file, \"r\"))\n",
    "    if isinstance(zero_scrolls_data, dict):\n",
    "        zero_scrolls_data = list(zero_scrolls_data.values())\n",
    "    compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "    load_key = \"compressed_prompt\" if compression else \"prompt\"\n",
    "    in_tokens = 0\n",
    "    for sample in zero_scrolls_data:\n",
    "        prompt = sample[load_key]\n",
    "        in_tokens += len(tokenizer.encode(prompt))\n",
    "    in_tokens_total_zs[model] += in_tokens\n",
    "    print(f\"Tokens in: {in_tokens:,}\")\n",
    "    file_dir = os.path.dirname(data_file)\n",
    "    answer_file = f\"{file_dir}/{target_model}/answer_{target}.json\"\n",
    "    res_data = list(json.load(open(answer_file, \"r\")).values())\n",
    "    out_tokens = sum(len(tokenizer.encode(d[\"pred\"])) for d in res_data)\n",
    "    out_tokens_total_zs[model] += out_tokens\n",
    "    print(f\"Tokens out: {out_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_scrolls_cost(print_res = False):\n",
    "    cost_origin = get_cost(in_tokens_total_zs[\"origin\"], out_tokens_total_zs[\"origin\"])\n",
    "    cost_comp = get_cost(in_tokens_total_zs[\"llmlingua2\"], out_tokens_total_zs[\"llmlingua2\"])\n",
    "    cost_comp_small = get_cost(in_tokens_total_zs[\"llmlingua2_small\"], out_tokens_total_zs[\"llmlingua2_small\"])\n",
    "    # cost_zero_shot = get_cost(in_tokens_total[\"zero_shot\"], out_tokens_total[\"zero_shot\"])\n",
    "\n",
    "    total_cost_zero_scrolls = cost_comp + cost_origin  #+ cost_zero_shot\n",
    "    cost_comp_3k = get_cost(808268, 30872)\n",
    "    if print_res:\n",
    "        print(\"Target model:\", MODEL)\n",
    "        print(f\"\\nTotal original cost: ${cost_origin:.2f}\")\n",
    "        print(f\"Total comp cost: ${cost_comp:.2f} - savings: ${cost_origin - cost_comp:.2f}\")\n",
    "        print(f\"Total comp cost (small): ${cost_comp_small:.2f} - savings: ${cost_origin - cost_comp_small:.2f}\")\n",
    "        # print(f\"Total zero-shot cost: ${cost_zero_shot:.2f} - savings: ${cost_origin - cost_zero_shot:.2f}\")\n",
    "        print(f\"\\nTotal repro cost: ${total_cost_zero_scrolls:.2f}\")\n",
    "        print(f\"Total comp cost (3k): ${cost_comp_3k:.2f} - savings: ${cost_origin - cost_comp_3k:.2f}\")\n",
    "    \n",
    "    return total_cost_zero_scrolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model: gpt-4o\n",
      "\n",
      "Total original cost: $6.83\n",
      "Total comp cost: $2.30 - savings: $4.52\n",
      "Total comp cost (small): $4.02 - savings: $2.81\n",
      "\n",
      "Total repro cost: $9.13\n",
      "Total comp cost (3k): $2.33 - savings: $4.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.1269175"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_zero_scrolls_cost(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate GSM8K repro gpt-3.5 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== origin: gsm8k_cot_example ======\n",
      "Tokens in: 3,190,968\n",
      "Tokens out: 216,268\n",
      "\n",
      "====== llmlingua2: target400_gsm8k_cot_example ======\n",
      "Tokens in: 682,230\n",
      "Tokens out: 248,501\n",
      "\n",
      "====== llmlingua2_small: target160_gsm8k_cot_example ======\n",
      "Tokens in: 318,186\n",
      "Tokens out: 245,343\n",
      "\n",
      "====== llmlingua2_small: target400_gsm8k_cot_example ======\n",
      "Tokens in: 703,334\n",
      "Tokens out: 248,785\n",
      "\n",
      "====== zero_shot: zero_shot ======\n",
      "Tokens in: 87,361\n",
      "Tokens out: 281,328\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "target_model = \"llama_3.1_70b_answer\"\n",
    "data_files_gsm8k = [\n",
    "    \"../results/gsm8k/origin/gsm8k_cot_example.json\",\n",
    "    # \"../results/gsm8k/llmlingua2/compression_target160_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/llmlingua2/compression_target400_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/llmlingua2_small/compression_target160_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/llmlingua2_small/compression_target400_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/zero_shot/zero_shot.json\",  # This file does not exist\n",
    "]\n",
    "\n",
    "\n",
    "def build_prompt_gsm8k(q, demonstrations):\n",
    "    query = f\"Question: {q}\" + \"\\nLet's think step by step.\"\n",
    "    return (\n",
    "        query\n",
    "        if not demonstrations\n",
    "        else f\"Please reference the following examples to answer the math question. \\n {demonstrations}\\n\\n\" + query\n",
    "    )\n",
    "\n",
    "\n",
    "in_tokens_total_gsm8k, out_tokens_total_gsm8k = defaultdict(int), defaultdict(int)\n",
    "for scenario in data_files_gsm8k:\n",
    "    get_split_name = lambda path: list(map(lambda part: part.replace(\".json\", \"\"), path.split(\"/\")))\n",
    "    filename_split = get_split_name(scenario)\n",
    "    model, target = filename_split[-2], filename_split[-1].replace(\"compression_\", \"\")\n",
    "    print(f\"\\n====== {model}: {target} ======\")\n",
    "    compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "    demonstrations = \"\"\n",
    "    if model != \"zero_shot\":\n",
    "        dem_list = json.load(open(scenario, \"r\"))\n",
    "        if isinstance(dem_list, dict):\n",
    "            dem_list = list(dem_list.values())\n",
    "        dem_list = dem_list[0][\"compressed_prompt_list\" if compression else \"prompt_list\"]\n",
    "        demonstrations = \"\\n\\n\".join([\"Question: \" + dem for dem in dem_list])\n",
    "\n",
    "    file_dir = os.path.dirname(scenario)\n",
    "    answer_file = f\"{file_dir}/{target_model}/answer_{target}.json\"\n",
    "    res_data = list(json.load(open(answer_file, \"r\")).values())\n",
    "    in_tokens = 0\n",
    "    for sample in res_data:\n",
    "        prompt = build_prompt_gsm8k(sample[\"question\"], demonstrations)\n",
    "        in_tokens += len(tokenizer.encode(prompt))\n",
    "    in_tokens_total_gsm8k[model] += in_tokens\n",
    "    print(f\"Tokens in: {in_tokens:,}\")\n",
    "\n",
    "    out_tokens = sum(len(tokenizer.encode(d[\"model_answer\"])) for d in res_data)\n",
    "    out_tokens_total_gsm8k[model] += out_tokens\n",
    "    print(f\"Tokens out: {out_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gsm8k_cost(print_res = False):\n",
    "    cost_origin = get_cost(in_tokens_total_gsm8k[\"origin\"], out_tokens_total_gsm8k[\"origin\"])\n",
    "    cost_comp = get_cost(in_tokens_total_gsm8k[\"llmlingua2\"], out_tokens_total_gsm8k[\"llmlingua2\"])\n",
    "    cost_comp_small = get_cost(in_tokens_total_gsm8k[\"llmlingua2_small\"], out_tokens_total_gsm8k[\"llmlingua2_small\"])\n",
    "    cost_zero_shot = get_cost(in_tokens_total_gsm8k[\"zero_shot\"], out_tokens_total_gsm8k[\"zero_shot\"])\n",
    "\n",
    "    total_repro_cost_gsm8k = cost_comp + cost_origin + cost_zero_shot\n",
    "    cost_comp_1_shot = get_cost(686804, 250652)\n",
    "    if print_res:\n",
    "        print(\"Target model:\", MODEL)\n",
    "        print(f\"\\nTotal original cost: ${cost_origin:.2f}\")\n",
    "        print(f\"Total comp cost: ${cost_comp:.2f} - savings: ${cost_origin - cost_comp:.2f}\")\n",
    "        print(f\"Total comp cost (small): ${cost_comp_small:.2f} - savings: ${cost_origin - cost_comp_small:.2f}\")\n",
    "        print(f\"Total zero-shot cost: ${cost_zero_shot:.2f} - savings: ${cost_origin - cost_zero_shot:.2f}\")\n",
    "        print(f\"\\nTotal repro cost: ${total_repro_cost_gsm8k:.2f}\")\n",
    "        print(f\"Total comp cost (1-shot): ${cost_comp_1_shot:.2f} - savings: ${cost_origin - cost_comp_1_shot:.2f}\")\n",
    "\n",
    "    return total_repro_cost_gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model: gpt-4o\n",
      "\n",
      "Total original cost: $10.14\n",
      "Total comp cost: $4.19 - savings: $5.95\n",
      "Total comp cost (small): $7.50 - savings: $2.65\n",
      "Total zero-shot cost: $3.03 - savings: $7.11\n",
      "\n",
      "Total repro cost: $17.36\n",
      "Total comp cost (1-shot): $4.22 - savings: $5.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.3623675"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_gsm8k_cost(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate BBH repro gpt-3.5 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== origin: bbh_cot_examples ======\n",
      "Tokens in: 5,674,570 (avg: 871.5358623867302), Tokens out: 1,345,994 (avg: 206.726155736446) - Cost: $27.65\n",
      "\n",
      "====== llmlingua2: target200_bbh_cot_examples ======\n",
      "Tokens in: 1,812,222 (avg: 278.33236062048843), Tokens out: 1,422,850 (avg: 218.53017969589925) - Cost: $18.76\n",
      "Ratio: 3.13\n",
      "Saving tokens: 68.06% / -5.71%\n",
      "Saving cost: 32.15%\n",
      "\n",
      "====== llmlingua2: target300_bbh_cot_examples ======\n",
      "Tokens in: 2,311,556 (avg: 355.0231915220396), Tokens out: 1,414,965 (avg: 217.31915220396252) - Cost: $19.93\n",
      "Ratio: 2.45\n",
      "Saving tokens: 59.26% / -5.12%\n",
      "Saving cost: 27.92%\n",
      "\n",
      "====== llmlingua2_small: target200_bbh_cot_examples ======\n",
      "Tokens in: 1,894,841 (avg: 291.02150207341424), Tokens out: 1,433,199 (avg: 220.11964367992627) - Cost: $19.07\n",
      "Ratio: 2.99\n",
      "Saving tokens: 66.61% / -6.48%\n",
      "Saving cost: 31.02%\n",
      "\n",
      "====== llmlingua2_small: target300_bbh_cot_examples ======\n",
      "Tokens in: 2,458,865 (avg: 377.64782675472276), Tokens out: 1,471,605 (avg: 226.01827676240208) - Cost: $20.86\n",
      "Ratio: 2.31\n",
      "Saving tokens: 56.67% / -9.33%\n",
      "Saving cost: 24.54%\n",
      "\n",
      "====== zero_shot: bbh_cot_examples ======\n",
      "Tokens in: 780,219 (avg: 119.830901551221), Tokens out: 1,535,248 (avg: 235.79296575026876) - Cost: $17.30\n",
      "Ratio: 7.27\n",
      "Saving tokens: 86.25% / -14.06%\n",
      "Saving cost: 37.41%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "target_model = \"llama_3.1_70b_answer\"\n",
    "data_files_bbh = {\n",
    "    \"origin\": \"../results/bbh/origin/bbh_cot_examples.json\",\n",
    "    \"llmlingua2_200\": \"../results/bbh/llmlingua2/compression_target200_bbh_cot_examples.json\",\n",
    "    \"llmlingua2_300\": \"../results/bbh/llmlingua2/compression_target300_bbh_cot_examples.json\",\n",
    "    \"llmlingua2_small_200\": \"../results/bbh/llmlingua2_small/compression_target200_bbh_cot_examples.json\",\n",
    "    \"llmlingua2_small_300\": \"../results/bbh/llmlingua2_small/compression_target300_bbh_cot_examples.json\",\n",
    "    \"zero-shot\": \"../results/bbh/zero_shot/bbh_cot_examples.json\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt_bbh(sample, task, prompts, instructions):\n",
    "    cot_prompt = prompts[task]\n",
    "    instruction = instructions[task]\n",
    "\n",
    "    if cot_prompt and cot_prompt[0] != \"\\n\":\n",
    "        cot_prompt = \"\\n\\n\" + cot_prompt\n",
    "    return f\"{instruction}{cot_prompt}\\n\\nQ: {sample['question']}\\nA: Let's think step by step.\\n\"\n",
    "\n",
    "\n",
    "bbh_dataset = json.load(open(\"../results/bbh/origin/bbh.json\", \"r\"))\n",
    "in_tokens_total_bbh, out_tokens_total_bbh = defaultdict(int), defaultdict(int)\n",
    "in_tokens, out_tokens = defaultdict(int), defaultdict(int)\n",
    "for scenario, filepath in data_files_bbh.items():\n",
    "    get_split_name = lambda path: list(map(lambda part: part.replace(\".json\", \"\"), path.split(\"/\")))\n",
    "    filename_split = get_split_name(filepath)\n",
    "    model, target = filename_split[-2], filename_split[-1].replace(\"compression_\", \"\")\n",
    "    file_dir = os.path.dirname(filepath)\n",
    "    answer_file = f\"{file_dir}/{target_model}/answer_{target}.json\"\n",
    "    if not os.path.exists(filepath) or not os.path.exists(answer_file):\n",
    "        continue\n",
    "    print(f\"\\n====== {model}: {target} ======\")\n",
    "    compression = model in [\"llmlingua2\", \"llmlingua2_small\"]\n",
    "    load_key = None if model == \"zero_shot\" else \"compressed_prompt\" if compression else \"prompt\"\n",
    "\n",
    "    demonstrations = json.load(open(filepath))\n",
    "    if isinstance(demonstrations, dict):\n",
    "        demonstrations = list(demonstrations.values())\n",
    "    prompts, instructions = {}, {}\n",
    "    for demon in demonstrations:\n",
    "        task = demon[\"task\"]\n",
    "        prompt = demon[load_key] if load_key is not None else \"\"\n",
    "        instructions[task] = demon[\"instruction\"]\n",
    "        prompts[task] = prompt\n",
    "\n",
    "    res_data = list(json.load(open(answer_file, \"r\")).values())\n",
    "    for i, sample in enumerate(res_data):\n",
    "        task = bbh_dataset[i][\"task\"]\n",
    "        prompt = build_prompt_bbh(sample, task, prompts, instructions)\n",
    "        in_tokens[scenario] += len(tokenizer.encode(prompt))\n",
    "    in_tokens_total_bbh[model] += in_tokens[scenario]\n",
    "    out_tokens[scenario] = sum(len(tokenizer.encode(d[\"model_answer\"])) for d in res_data)\n",
    "    out_tokens_total_bbh[model] += out_tokens[scenario]\n",
    "    cost_scenario = get_cost(in_tokens[scenario], out_tokens[scenario])\n",
    "    print_str = f\"Tokens in: {in_tokens[scenario]:,} (avg: {in_tokens[scenario] / 6511:,}), Tokens out: {out_tokens[scenario]:,} (avg: {out_tokens[scenario] / 6511:,}) - Cost: ${cost_scenario:.2f}\"\n",
    "    if scenario != \"origin\":\n",
    "        print_str += f\"\\nRatio: {in_tokens['origin'] / in_tokens[scenario]:.2f}\"\n",
    "        print_str += f'\\nSaving tokens: {(1 - in_tokens[scenario] / in_tokens[\"origin\"]) * 100:.2f}% / {(1 - out_tokens[scenario] / out_tokens[\"origin\"]) * 100:.2f}%'\n",
    "        print_str += f'\\nSaving cost: {(1 - cost_scenario / cost_origin) * 100:.2f}%'\n",
    "    else:\n",
    "        cost_origin = cost_scenario\n",
    "    print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbh_cost(print_res = False):\n",
    "    cost_origin = get_cost(in_tokens_total_bbh[\"origin\"], out_tokens_total_bbh[\"origin\"])\n",
    "    cost_comp = get_cost(in_tokens_total_bbh[\"llmlingua2\"], out_tokens_total_bbh[\"llmlingua2\"])\n",
    "    cost_comp_small = get_cost(in_tokens_total_bbh[\"llmlingua2_small\"], out_tokens_total_bbh[\"llmlingua2_small\"])\n",
    "    cost_zero_shot = get_cost(in_tokens_total_bbh[\"zero_shot\"], out_tokens_total_bbh[\"zero_shot\"])\n",
    "\n",
    "    total_repro_cost_bbh = cost_comp + cost_origin + cost_zero_shot\n",
    "    cost_comp_1_shot = get_cost(2339622, 1438517)\n",
    "    if print_res:\n",
    "        print(\"Target model:\", MODEL)\n",
    "        print(f\"\\nTotal original cost: ${cost_origin:.2f}\")\n",
    "        print(f\"Total comp cost: ${cost_comp:.2f} - savings: ${cost_origin - cost_comp:.2f}\")\n",
    "        print(f\"Total comp cost (small): ${cost_comp_small:.2f} - savings: ${cost_origin - cost_comp_small:.2f}\")\n",
    "        print(f\"Total zero-shot cost: ${cost_zero_shot:.2f} - savings: ${cost_origin - cost_zero_shot:.2f}\")\n",
    "        print(f\"\\nTotal repro cost: ${total_repro_cost_bbh:.2f}\")\n",
    "        print(f\"Total comp cost (1-shot): ${cost_comp_1_shot:.2f} - savings: ${cost_origin - cost_comp_1_shot:.2f}\")\n",
    "    return total_repro_cost_bbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model: gpt-4o\n",
      "\n",
      "Total original cost: $27.65\n",
      "Total comp cost: $19.93 - savings: $7.72\n",
      "Total comp cost (small): $39.93 - savings: $-12.29\n",
      "Total zero-shot cost: $17.30 - savings: $10.34\n",
      "\n",
      "Total repro cost: $64.88\n",
      "Total comp cost (1-shot): $20.23 - savings: $7.41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.8779325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_bbh_cost(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total repro cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4o \n",
      "\n",
      "MeetingBench: \t$36.50 \t\t (Batch: $18.25)\n",
      "LongBench: \t$160.86 \t (Batch: $80.43)\n",
      "Zero Scrolls: \t$9.13 \t\t (Batch: $4.56)\n",
      "GSM8K: \t\t$17.36 \t\t (Batch: $8.68)\n",
      "BBH: \t\t$64.88 \t\t (Batch: $32.44)\n",
      "\n",
      "Total: \t\t$288.72\n",
      "Batch: \t\t$144.36\n"
     ]
    }
   ],
   "source": [
    "# Get total repro cost\n",
    "print(\"Model:\", MODEL, \"\\n\")\n",
    "cost_meetingbench = calc_meetingbank_cost()\n",
    "print(f\"MeetingBench: \\t${cost_meetingbench:.2f} \\t\\t (Batch: ${cost_meetingbench / 2:.2f})\")\n",
    "cost_longbench = calc_longbench_cost()\n",
    "print(f\"LongBench: \\t${cost_longbench:.2f} \\t (Batch: ${cost_longbench / 2:.2f})\")\n",
    "cost_zero_scrolls = calc_zero_scrolls_cost()\n",
    "print(f\"Zero Scrolls: \\t${cost_zero_scrolls:.2f} \\t\\t (Batch: ${cost_zero_scrolls / 2:.2f})\")\n",
    "cost_gsm8k = calc_gsm8k_cost()\n",
    "print(f\"GSM8K: \\t\\t${cost_gsm8k:.2f} \\t\\t (Batch: ${cost_gsm8k / 2:.2f})\")\n",
    "cost_bbh = calc_bbh_cost()\n",
    "print(f\"BBH: \\t\\t${cost_bbh:.2f} \\t\\t (Batch: ${cost_bbh / 2:.2f})\")\n",
    "\n",
    "total_repro_cost = cost_meetingbench + cost_longbench + cost_zero_scrolls + cost_gsm8k + cost_bbh\n",
    "print(f\"\\nTotal: \\t\\t${total_repro_cost:.2f}\\nBatch: \\t\\t${total_repro_cost / 2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-3.5 e2e Benchmark Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 10,505,700.0\n",
      "Output tokens: 145,080\n",
      "Requests: 3900\n",
      "Cost: $1.66\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "# gpt-3.5-turbo-0125\n",
    "COST_IN = 0.5\n",
    "COST_OUT = 1.5\n",
    "# gpt-4o-mini\n",
    "COST_IN = 0.15\n",
    "COST_OUT = 0.6\n",
    "\n",
    "input_sizes = [100, 250, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 12000, 16000]  # Input token sizes\n",
    "rates = [1, 2 / 3, 1 / 2, 1 / 3, 1 / 5]  # 1x, 1.5x, 2x, 3x, 5x compression rates\n",
    "output_sizes = [1, 10, 25, 50, 100]\n",
    "iterations = 12\n",
    "\n",
    "in_tokens, out_tokens = [], []\n",
    "requests = 0\n",
    "\n",
    "for output_size, input_size, rate in (prog := itertools.product(output_sizes, input_sizes, rates)):\n",
    "    in_tokens.append(input_size * rate * iterations)\n",
    "    out_tokens.append(output_size * iterations)\n",
    "    requests += 12\n",
    "\n",
    "print(f\"Input tokens: {sum(in_tokens):,}\")\n",
    "print(f\"Output tokens: {sum(out_tokens):,}\")\n",
    "print(f\"Requests: {requests}\")\n",
    "print(f\"Cost: ${sum(in_tokens) / 1_000_000 * COST_IN + sum(out_tokens) / 1_000_000 * COST_OUT:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Tables Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "models = [\"mistral7b_answer\", \"llama_3.1_70b_answer\", \"command_r_plus_answer\", \"gpt_4o_answer\", \"gpt_4o_mini_answer\"]\n",
    "scenarios = [\"llmlingua2_small\", \"llmlingua2\", \"origin\"]\n",
    "\n",
    "# Meetingbank\n",
    "outdir = \"../../ma-thesis/tables/meetingbank/\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "columns = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        (\"QA\", \"EM\"),\n",
    "        (\"Summary\", \"BLEU\"),\n",
    "        (\"Summary\", \"ROUGE-1\"),\n",
    "        (\"Summary\", \"ROUGE-2\"),\n",
    "        (\"Summary\", \"ROUGE-L\"),\n",
    "        (\"Summary\", \"BERTScore\"),\n",
    "        (\"Tokens\", \"\"),\n",
    "        (\"Ratio\", \"\")\n",
    "    ]\n",
    ")\n",
    "index = [\"LLMLingua-2-small\", \"LLMLingua-2\", \"Original\"]\n",
    "df = pd.DataFrame(columns=columns, index=index)\n",
    "size_and_ratio = [[895, 3.4], [982, 3.1], [3004, 1.0]]\n",
    "\n",
    "for model in models:\n",
    "    df_m = df.copy()\n",
    "    rows = []\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        qa_file = \"metrics_meetingbank_QA\" if scenario == \"origin\" else \"metrics_ratio33_meetingbank_QA\"\n",
    "        sum_file = \"metrics_meetingbank_summary\" if scenario == \"origin\" else \"metrics_ratio33_meetingbank_summary\"\n",
    "        qa_path = f\"../results/meetingbank_short/{scenario}/{model}/{qa_file}.json\"\n",
    "        sum_path = f\"../results/meetingbank_short/{scenario}/{model}/{sum_file}.json\"\n",
    "        if not os.path.exists(qa_path) or not os.path.exists(sum_path):\n",
    "            rows.append([None] * 8)\n",
    "            continue\n",
    "        qa = list(json.load(open(qa_path, \"r\")).values())[0]\n",
    "        summary = np.array([*json.load(open(sum_path, \"r\")).values()]) * 100\n",
    "        summary = [*summary[:4], summary[-1]]\n",
    "        rows.append([qa, *summary, *size_and_ratio[i]])\n",
    "    df_m.loc[:, :] = rows\n",
    "    df_m = df_m.dropna(how='all')\n",
    "    df_m[(\"Tokens\", \"\")] = df_m[(\"Tokens\", \"\")].map(lambda x: f\"{x:,}\")\n",
    "    df_m[(\"Ratio\", \"\")] = df_m[(\"Ratio\", \"\")].map(lambda x: f\"{x:.1f}x\" if isinstance(x, float) else x)\n",
    "    tex_table = df_m.to_latex(float_format=\"%.2f\", multicolumn_format=\"c|\", column_format=\"@{}l|c|ccccc|cc@{}\", index_names=False)\n",
    "    for match in [\"QA\", \"Summary\", \"Tokens\", \"Ratio\"]:\n",
    "        tex_table = tex_table.replace(match, f\"\\\\textbf{{{match}}}\")\n",
    "    tex_table = tex_table.replace(\"\\\\\\\\\\nOrig\", \"\\\\\\\\\\\\midrule\\nOrig\", 1)\n",
    "    with open(f'{outdir}/{model.split(\"_answer\")[0]}.tex', \"w\") as f:\n",
    "        f.write(tex_table)\n",
    "    # print(f\"\\nModel: {model}\")\n",
    "    # display(df_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSM8K\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "models = [\"mistral7b_answer\", \"llama_3.1_70b_answer\", \"command_r_plus_answer\", \"gpt_4o_answer\", \"gpt_4o_mini_answer\"]\n",
    "scenarios = [\n",
    "    \"../results/gsm8k/origin/{model}/metrics_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/llmlingua2/{model}/metrics_target400_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/llmlingua2_small/{model}/metrics_target400_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/llmlingua2/{model}/metrics_target160_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/llmlingua2_small/{model}/metrics_target160_gsm8k_cot_example.json\",\n",
    "    \"../results/gsm8k/zero_shot/{model}/metrics_zero_shot.json\",\n",
    "]\n",
    "\n",
    "outdir = \"../../ma-thesis/tables/gsm8k/\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "columns = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        (\"Original\", \"\"),\n",
    "        (\"Target 400\", \"LLMLingua-2\"),\n",
    "        (\"Target 400\", \"LLMLingua-2-small\"),\n",
    "        (\"Target 160\", \"LLMLingua-2\"),\n",
    "        (\"Target 160\", \"LLMLingua-2-small\"),\n",
    "        (\"Zero-Shot\", \"\"),\n",
    "    ]\n",
    ")\n",
    "index = [\n",
    "    \"Score\",\n",
    "    # \"Tokens\",\n",
    "    # \"Ratio\",\n",
    "]\n",
    "df = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "for model in models:\n",
    "    df_m = df.copy()\n",
    "    rows = [\n",
    "        [],\n",
    "        # [2366, 440, 455, 157, 159, \"---\"],\n",
    "        # [1.0, 5.4, 5.2, 15.1, 14.9, \"---\"],\n",
    "    ]\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        res_file = scenario.format(model=model)\n",
    "        if not os.path.exists(res_file):\n",
    "            rows[0].append(None)\n",
    "            continue\n",
    "        score = json.load(open(res_file, \"r\"))[\"score\"] * 100\n",
    "        rows[0].append(score)\n",
    "    df_m.loc[:, :] = rows\n",
    "    df_m = df_m.dropna(how=\"any\", axis=1)\n",
    "    # df_m.loc[\"Ratio\"] = df_m.loc[\"Ratio\"].map(lambda x: f\"{x:.1f}x\" if isinstance(x, float) else x)\n",
    "    # print(f\"\\nModel: {model}\")\n",
    "    # display(df_m)\n",
    "\n",
    "    tex_table = df_m.to_latex(\n",
    "        float_format=\"%.2f\",\n",
    "        multicolumn_format=\"c|\",\n",
    "        column_format=\"@{}l|c|cc|cc|c@{}\" if len(df_m.columns) > 3 else \"@{}l|c|c|c@{}\",\n",
    "        index_names=False,\n",
    "    )\n",
    "    for match in [\"Original\", \"Target 400\", \"Target 160\", \"Zero-Shot\"]:\n",
    "        tex_table = tex_table.replace(match, f\"\\\\textbf{{{match}}}\")\n",
    "    tex_table = tex_table.replace(\"\\\\\\\\\\nTokens\", \"\\\\\\\\\\\\midrule\\nTokens\", 1)\n",
    "    with open(f'{outdir}/{model.split(\"_answer\")[0]}.tex', \"w\") as f:\n",
    "        f.write(tex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LongBench / ZeroScrolls\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "models = [\"mistral7b_answer\", \"llama_3.1_70b_answer\", \"command_r_plus_answer\", \"gpt_4o_answer\", \"gpt_4o_mini_answer\"]\n",
    "lb_categories = {\n",
    "    \"Single-doc QA\": [\"multifieldqa_en\", \"multifieldqa_zh\", \"narrativeqa\", \"qasper\"],\n",
    "    \"Multi-doc QA\": [\"hotpotqa\", \"2wikimqa\", \"musique\", \"dureader\"],\n",
    "    \"Summarization\": [\"gov_report\", \"qmsum\", \"multi_news\", \"vcsum\"],\n",
    "    \"Few-Shot\": [\"triviaqa\", \"samsum\", \"trec\", \"lsht\"],\n",
    "    \"Synthetic\": [\"passage_count\", \"passage_retrieval_en\", \"passage_retrieval_zh\"],\n",
    "    \"Code\": [\"lcc\", \"repobench-p\"],\n",
    "}\n",
    "lb_tasknames = {\n",
    "    \"narrativeqa\": \"NarrativeQA\",\n",
    "    \"qasper\": \"Qasper\",\n",
    "    \"multifieldqa_en\": \"MultiFieldQA (EN)\",\n",
    "    \"multifieldqa_zh\": \"MultiFieldQA (ZH)\",\n",
    "    \"hotpotqa\": \"HotpotQA\",\n",
    "    \"2wikimqa\": \"2WikiMultihopQA\",\n",
    "    \"musique\": \"MuSiQue\",\n",
    "    \"dureader\": \"DuReader (ZH)\",\n",
    "    \"gov_report\": \"GovReport\",\n",
    "    \"qmsum\": \"QMSum\",\n",
    "    \"multi_news\": \"MultiNews\",\n",
    "    \"vcsum\": \"VCSUM (ZH)\",\n",
    "    \"trec\": \"TREC\",\n",
    "    \"triviaqa\": \"TriviaQA\",\n",
    "    \"samsum\": \"SAMSum\",\n",
    "    \"lsht\": \"LSHT (ZH)\",\n",
    "    \"passage_retrieval_en\": \"PassageRetrieval (EN)\",\n",
    "    \"passage_count\": \"PassageCount\",\n",
    "    \"passage_retrieval_zh\": \"PassageRetrieval (ZH)\",\n",
    "    \"lcc\": \"LCC\",\n",
    "    \"repobench-p\": \"RepoBench-P\",\n",
    "    \"avg\": \"Average\",\n",
    "}\n",
    "zero_scrolls_tasknames = {\n",
    "    \"gov_report\": \"GovReport\",\n",
    "    \"summ_screen_fd\": \"SummScreenFD\",\n",
    "    \"qmsum\": \"QMSum\",\n",
    "    \"qasper\": \"Qasper\",\n",
    "    \"narrative_qa\": \"NarrativeQA\",\n",
    "    \"quality\": \"QuALITY\",\n",
    "    \"musique\": \"MuSiQue\",\n",
    "    \"squality\": \"SQuALITY\",\n",
    "    \"space_digest\": \"SpaceDigest\",\n",
    "    \"book_sum_sort\": \"BookSumSort\",\n",
    "    \"avg\": \"Average\",\n",
    "}\n",
    "zero_scrolls_categories = {\n",
    "    \"Summarization\": [\"gov_report\", \"qmsum\", \"summ_screen_fd\", \"squality\"],\n",
    "    \"QA\": [\"qasper\", \"narrative_qa\", \"musique\", \"quality\"],\n",
    "    \"Data Processing\": [\"space_digest\", \"book_sum_sort\"],\n",
    "}\n",
    "bbh_tasknames = {\n",
    "    \"multistep_arithmetic_two\": \"Multi-Step Arithmetic\",\n",
    "    \"boolean_expressions\": \"Boolean Expressions\",\n",
    "    \"logical_deduction_three_objects\": \"Logical Deduction (3 Obj.)\",\n",
    "    \"logical_deduction_five_objects\": \"Logical Deduction (5 Obj.)\",\n",
    "    \"logical_deduction_seven_objects\": \"Logical Deduction (7 Obj.)\",\n",
    "    \"geometric_shapes\": \"Geometric Shapes\",\n",
    "    \"dyck_languages\": \"Dyck Languages\",\n",
    "    \"navigate\": \"Navigate\",\n",
    "    \"temporal_sequences\": \"Temporal Sequences\",\n",
    "    \"formal_fallacies\": \"Formal Fallacies\",\n",
    "    \"object_counting\": \"Object Counting\",\n",
    "    \"penguins_in_a_table\": \"Penguins in a Table\",\n",
    "    \"tracking_shuffled_objects_three_objects\": \"Track. 3 Shuffled Obj.\",\n",
    "    \"tracking_shuffled_objects_five_objects\": \"Track. 5 Shuffled Obj.\",\n",
    "    \"tracking_shuffled_objects_seven_objects\": \"Track. 7 Shuffled Obj.\",\n",
    "    \"reasoning_about_colored_objects\": \"Reasoning about Col. Obj.\",\n",
    "    \"web_of_lies\": \"Web of Lies\",\n",
    "    \"word_sorting\": \"Word Sorting\",\n",
    "    \"disambiguation_qa\": \"Disambiguation QA\",\n",
    "    \"hyperbaton\": \"Hyperbaton\",\n",
    "    \"salient_translation_error_detection\": \"Salient Transl. Err. Detection\",\n",
    "    \"snarks\": \"Snarks\",\n",
    "    \"sports_understanding\": \"Sports Understanding\",\n",
    "    \"movie_recommendation\": \"Movie Recommendation\",\n",
    "    \"date_understanding\": \"Date Understanding\",\n",
    "    \"causal_judgement\": \"Causal Judgement\",\n",
    "    \"ruin_names\": \"Ruin Names\",\n",
    "    \"avg\": \"Average\",\n",
    "}\n",
    "benchmarks = {\n",
    "    \"longbench\": {\n",
    "        \"scen\": [\n",
    "            \"../results/longbench/origin/{model}/metrics_longbench_test_formatted.json\",\n",
    "            \"../results/longbench/llmlingua2/{model}/metrics_target3000_longbench_test_formatted.json\",\n",
    "            \"../results/longbench/llmlingua2_small/{model}/metrics_target3000_longbench_test_formatted.json\",\n",
    "            \"../results/longbench/llmlingua2/{model}/metrics_target2000_longbench_test_formatted.json\",\n",
    "            \"../results/longbench/llmlingua2_small/{model}/metrics_target2000_longbench_test_formatted.json\",\n",
    "            \"../results/longbench/zero_shot/{model}/metrics_longbench_test_formatted.json\",\n",
    "        ],\n",
    "        \"tasks\": [t for ts in lb_categories.values() for t in ts] + [\"avg\"],\n",
    "        \"tasknames\": lb_tasknames,\n",
    "        \"key\": \"score\",\n",
    "        \"targets\": [\"Target 3000\", \"Target 2000\"],\n",
    "        \"zero_shot\": [\"triviaqa\", \"samsum\", \"trec\", \"lsht\", \"repobench-p\"],\n",
    "    },\n",
    "    \"zero_scrolls\": {\n",
    "        \"scen\": [\n",
    "            \"../results/zero_scrolls/origin/{model}/metrics_zero_scrolls_validation.json\",\n",
    "            \"../results/zero_scrolls/llmlingua2/{model}/metrics_target3000_zero_scrolls_validation.json\",\n",
    "            \"../results/zero_scrolls/llmlingua2_small/{model}/metrics_target3000_zero_scrolls_validation.json\",\n",
    "            \"../results/zero_scrolls/llmlingua2/{model}/metrics_target2000_zero_scrolls_validation.json\",\n",
    "            \"../results/zero_scrolls/llmlingua2_small/{model}/metrics_target2000_zero_scrolls_validation.json\",\n",
    "        ],\n",
    "        \"tasks\": [t for ts in zero_scrolls_categories.values() for t in ts] + [\"avg\"],\n",
    "        \"tasknames\": zero_scrolls_tasknames,\n",
    "        \"key\": \"zero_scrolls_score\",\n",
    "        \"targets\": [\"Target 3000\", \"Target 2000\"],\n",
    "    },\n",
    "    \"bbh\": {\n",
    "        \"scen\": [\n",
    "            \"../results/bbh/origin/{model}/metrics_bbh_cot_examples.json\",\n",
    "            \"../results/bbh/llmlingua2/{model}/metrics_target300_bbh_cot_examples.json\",\n",
    "            \"../results/bbh/llmlingua2_small/{model}/metrics_target300_bbh_cot_examples.json\",\n",
    "            \"../results/bbh/llmlingua2/{model}/metrics_target200_bbh_cot_examples.json\",\n",
    "            \"../results/bbh/llmlingua2_small/{model}/metrics_target200_bbh_cot_examples.json\",\n",
    "            \"../results/bbh/zero_shot/{model}/metrics_bbh_cot_examples.json\",\n",
    "        ],\n",
    "        \"tasks\": list(bbh_tasknames.keys()),\n",
    "        \"tasknames\": bbh_tasknames,\n",
    "        \"key\": \"acc\",\n",
    "        \"targets\": [\"Target 300\", \"Target 200\"],\n",
    "        \"zero_shot\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    outdir = f\"../../ma-thesis/tables/{benchmark}\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    scen = benchmarks[benchmark][\"scen\"]\n",
    "    tasks = benchmarks[benchmark][\"tasks\"]\n",
    "    tasknames = benchmarks[benchmark][\"tasknames\"]\n",
    "    key = benchmarks[benchmark][\"key\"]\n",
    "    targets = benchmarks[benchmark][\"targets\"]\n",
    "\n",
    "    for model in models:\n",
    "        columns = [(\"Original\", \"\")]\n",
    "        for target in targets:\n",
    "            columns.append((target, \"LLMLingua-2\"))\n",
    "            columns.append((target, \"LLMLingua-2-small\"))\n",
    "        if \"zero_shot\" in benchmarks[benchmark]:\n",
    "            columns.append((\"Zero-Shot\", \"\"))\n",
    "\n",
    "        df = pd.DataFrame(index=tasks, columns=pd.MultiIndex.from_tuples(columns))\n",
    "\n",
    "        for s, col in zip(scen, df.columns):\n",
    "            scorepath = s.format(model=model)\n",
    "            if not os.path.exists(scorepath):\n",
    "                continue\n",
    "            scores = json.load(open(scorepath))\n",
    "            for task in tasks:\n",
    "                if \"zero_shot\" in s and task not in benchmarks[benchmark][\"zero_shot\"]:\n",
    "                    df.loc[task, col] = \"\"\n",
    "                else:\n",
    "                    df.loc[task, col] = (scores[task][key] if task != \"avg\" else scores[\"avg\"]) * (\n",
    "                        100 if key == \"acc\" else 1\n",
    "                    )\n",
    "\n",
    "        df = df.dropna(how=\"all\", axis=1)\n",
    "        df.index = df.index.map(lambda x: tasknames[x])\n",
    "        df = df.apply(\n",
    "            lambda row: (\n",
    "                row\n",
    "                if row.name != \"Average\"\n",
    "                else row.apply(lambda x: f\"\\\\textbf{{{x:.2f}}}\" if isinstance(x, float) else \"\")\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        column_format = \"l|c|cc|cc\" if not model.startswith(\"gpt_4o\") else \"l|c|c\"\n",
    "        tex_table = df.to_latex(\n",
    "            float_format=\"%.2f\",\n",
    "            multicolumn_format=\"c|\",\n",
    "            column_format=f'@{{}}{column_format + (\"|c\" if \"Zero-Shot\" in df.columns else \"\")}@{{}}',\n",
    "        )\n",
    "\n",
    "        if not \"zero_shot\" in benchmarks[benchmark]:\n",
    "            tex_table = tex_table.replace(\"|}{Target 2\", \"}{Target 2\")\n",
    "        tex_table = tex_table.replace(\"Average\", \"\\\\midrule Average\")\n",
    "        for match in [\"Average\", \"Original\", *targets, \"Zero-Shot\"]:\n",
    "            tex_table = tex_table.replace(match, f\"\\\\textbf{{{match}}}\")\n",
    "\n",
    "        with open(f'{outdir}/{model.split(\"_answer\")[0]}.tex', \"w\") as f:\n",
    "            f.write(tex_table)\n",
    "\n",
    "        # print(\"\\nModel:\", model)\n",
    "        # display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
